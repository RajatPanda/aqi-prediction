{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install opencage\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "TAnWF-3p1x2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb17824-7b05-43e4-937a-ec3e86892f00",
        "collapsed": true
      },
      "id": "TAnWF-3p1x2_",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: opencage in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: Requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from opencage) (2.32.3)\n",
            "Requirement already satisfied: backoff>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from opencage) (2.2.1)\n",
            "Requirement already satisfied: tqdm>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from opencage) (4.67.1)\n",
            "Requirement already satisfied: certifi>=2024.07.04 in /usr/local/lib/python3.11/dist-packages (from opencage) (2025.6.15)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.11/dist-packages (from opencage) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from geopy.geocoders import Nominatim\n",
        "from tqdm.notebook import tqdm\n",
        "from opencage.geocoder import OpenCageGeocode\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import warnings\n",
        "# Arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# LSTM\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# Random forest\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "KAmjdWX6zJSl"
      },
      "id": "KAmjdWX6zJSl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u51nzga5xoJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68ed8a7-892c-411c-8ac3-52509e5fedb9"
      },
      "id": "u51nzga5xoJg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ARIMAModel:\n",
        "    def __init__(self, order=(1, 1, 1)):\n",
        "        \"\"\"\n",
        "        Initialize ARIMA model for AQI forecasting\n",
        "\n",
        "        Parameters:\n",
        "        order: tuple (p, d, q) for ARIMA parameters\n",
        "        \"\"\"\n",
        "        self.order = order\n",
        "        self.model = None\n",
        "        self.fitted_model = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def check_stationarity(self, timeseries):\n",
        "        \"\"\"\n",
        "        Check if time series is stationary using ADF test\n",
        "        \"\"\"\n",
        "        result = adfuller(timeseries.dropna())\n",
        "        print('ADF Statistic:', result[0])\n",
        "        print('p-value:', result[1])\n",
        "        print('Critical Values:')\n",
        "        for key, value in result[4].items():\n",
        "            print(f'\\t{key}: {value}')\n",
        "\n",
        "        if result[1] <= 0.05:\n",
        "            print(\"Series is stationary\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Series is non-stationary\")\n",
        "            return False\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepare data for ARIMA modeling\n",
        "        \"\"\"\n",
        "        # Convert date column and set as index\n",
        "        df['From Date'] = pd.to_datetime(df['From Date'])\n",
        "        df = df.set_index('From Date')\n",
        "\n",
        "        # Sort by date\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Handle missing values in AQI\n",
        "        df['AQI'] = df['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        return df['AQI']\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"\n",
        "        Fit ARIMA model to training data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare time series data\n",
        "            ts_data = self.prepare_data(train_data)\n",
        "\n",
        "            # Check stationarity\n",
        "            self.check_stationarity(ts_data)\n",
        "\n",
        "            # Fit ARIMA model\n",
        "            self.model = ARIMA(ts_data, order=self.order)\n",
        "            self.fitted_model = self.model.fit()\n",
        "            self.is_fitted = True\n",
        "\n",
        "            print(f\"ARIMA{self.order} model fitted successfully\")\n",
        "            print(self.fitted_model.summary())\n",
        "\n",
        "            return self.fitted_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting ARIMA model: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def predict(self, steps=1):\n",
        "        \"\"\"\n",
        "        Make predictions using fitted ARIMA model\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        try:\n",
        "            # Make predictions\n",
        "            forecast = self.fitted_model.forecast(steps=steps)\n",
        "            conf_int = self.fitted_model.get_forecast(steps=steps).conf_int()\n",
        "\n",
        "            return forecast, conf_int\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate model performance\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before evaluation\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            ts_test = self.prepare_data(test_data)\n",
        "\n",
        "            # Make predictions\n",
        "            predictions, _ = self.predict(steps=len(ts_test))\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(ts_test.values, predictions)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(ts_test.values, predictions)\n",
        "\n",
        "            metrics = {\n",
        "                'MSE': mse,\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            print(f\"ARIMA Model Evaluation:\")\n",
        "            print(f\"RMSE: {rmse:.4f}\")\n",
        "            print(f\"MAE: {mae:.4f}\")\n",
        "\n",
        "            return metrics, predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating model: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the fitted model\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before saving\")\n",
        "\n",
        "        try:\n",
        "            model_data = {\n",
        "                'fitted_model': self.fitted_model,\n",
        "                'order': self.order,\n",
        "                'is_fitted': self.is_fitted\n",
        "            }\n",
        "            joblib.dump(model_data, filepath)\n",
        "            print(f\"ARIMA model saved to {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model: {str(e)}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Load a saved model\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_data = joblib.load(filepath)\n",
        "            self.fitted_model = model_data['fitted_model']\n",
        "            self.order = model_data['order']\n",
        "            self.is_fitted = model_data['is_fitted']\n",
        "            print(f\"ARIMA model loaded from {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")"
      ],
      "metadata": {
        "id": "h0WoPUPrEee8"
      },
      "id": "h0WoPUPrEee8",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel:\n",
        "    def __init__(self, sequence_length=24, lstm_units=50, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Initialize LSTM model for AQI prediction\n",
        "\n",
        "        Parameters:\n",
        "        sequence_length: Number of time steps to look back\n",
        "        lstm_units: Number of LSTM units\n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.regression_model = None\n",
        "        self.classification_model = None\n",
        "        self.severity_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.severity_encoder = LabelEncoder()\n",
        "        self.pollutant_encoder = LabelEncoder()\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepare data for LSTM modeling\n",
        "        \"\"\"\n",
        "        # Convert date and sort\n",
        "        df['From Date'] = pd.to_datetime(df['From Date'])\n",
        "        df = df.sort_values('From Date')\n",
        "\n",
        "        # Add time features\n",
        "        df['Hour'] = df['From Date'].dt.hour\n",
        "        df['Day'] = df['From Date'].dt.day\n",
        "        df['Month'] = df['From Date'].dt.month\n",
        "        df['Year'] = df['From Date'].dt.year\n",
        "        df['DayOfWeek'] = df['From Date'].dt.dayofweek\n",
        "\n",
        "        # Handle missing values\n",
        "        numeric_cols = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                        'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                        'WD (degree)', 'WS (m/s)']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        df['Severity_encoded'] = self.severity_encoder.fit_transform(df['Severity'])\n",
        "        df['Main_Pollutant_encoded'] = self.pollutant_encoder.fit_transform(df['Main Pollutant'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_sequences(self, data, target_col, feature_cols):\n",
        "        \"\"\"\n",
        "        Create sequences for LSTM input\n",
        "        \"\"\"\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(self.sequence_length, len(data)):\n",
        "            X.append(data[feature_cols].iloc[i-self.sequence_length:i].values)\n",
        "            y.append(data[target_col].iloc[i])\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def build_regression_model(self, n_features):\n",
        "        \"\"\"\n",
        "        Build LSTM model for AQI regression\n",
        "        \"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(self.lstm_units, return_sequences=True, input_shape=(self.sequence_length, n_features)),\n",
        "            Dropout(self.dropout_rate),\n",
        "            LSTM(self.lstm_units // 2, return_sequences=False),\n",
        "            Dropout(self.dropout_rate),\n",
        "            Dense(25, activation='relu'),\n",
        "            Dropout(self.dropout_rate),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    def build_classification_model(self, n_features, n_classes):\n",
        "        \"\"\"\n",
        "        Build LSTM model for classification\n",
        "        \"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(self.lstm_units, return_sequences=True, input_shape=(self.sequence_length, n_features)),\n",
        "            Dropout(self.dropout_rate),\n",
        "            LSTM(self.lstm_units // 2, return_sequences=False),\n",
        "            Dropout(self.dropout_rate),\n",
        "            Dense(25, activation='relu'),\n",
        "            Dropout(self.dropout_rate),\n",
        "            Dense(n_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, train_data, validation_data=None, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Fit LSTM models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_df = self.prepare_data(train_data.copy())\n",
        "\n",
        "            # Select features\n",
        "            feature_cols = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                           'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                           'WD (degree)', 'WS (m/s)', 'Hour', 'Day', 'Month', 'Year', 'DayOfWeek',\n",
        "                           'latitude', 'longitude', 'elevation']\n",
        "\n",
        "            # Filter available columns\n",
        "            available_features = [col for col in feature_cols if col in train_df.columns]\n",
        "\n",
        "            # Scale features\n",
        "            train_df[available_features] = self.scaler.fit_transform(train_df[available_features])\n",
        "\n",
        "            # Create sequences\n",
        "            X_train, y_aqi = self.create_sequences(train_df, 'AQI', available_features)\n",
        "            _, y_severity = self.create_sequences(train_df, 'Severity_encoded', available_features)\n",
        "            _, y_pollutant = self.create_sequences(train_df, 'Main_Pollutant_encoded', available_features)\n",
        "\n",
        "            if len(X_train) == 0:\n",
        "                raise ValueError(\"Not enough data to create sequences\")\n",
        "\n",
        "            # Build models\n",
        "            n_features = len(available_features)\n",
        "            self.regression_model = self.build_regression_model(n_features)\n",
        "            self.classification_model = self.build_classification_model(n_features,\n",
        "                                                                       len(np.unique(y_severity)))\n",
        "            self.severity_model = self.build_classification_model(n_features,\n",
        "                                                                 len(np.unique(y_pollutant)))\n",
        "\n",
        "            # Callbacks\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "            # Prepare validation data if provided\n",
        "            X_val, y_val_aqi, y_val_severity, y_val_pollutant = None, None, None, None\n",
        "            if validation_data is not None:\n",
        "                val_df = self.prepare_data(validation_data.copy())\n",
        "                val_df[available_features] = self.scaler.transform(val_df[available_features])\n",
        "                X_val, y_val_aqi = self.create_sequences(val_df, 'AQI', available_features)\n",
        "                _, y_val_severity = self.create_sequences(val_df, 'Severity_encoded', available_features)\n",
        "                _, y_val_pollutant = self.create_sequences(val_df, 'Main_Pollutant_encoded', available_features)\n",
        "\n",
        "            # Train AQI regression model\n",
        "            print(\"Training AQI regression model...\")\n",
        "            history_reg = self.regression_model.fit(\n",
        "                X_train, y_aqi,\n",
        "                validation_data=(X_val, y_val_aqi) if X_val is not None else None,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[early_stopping, reduce_lr] if X_val is not None else [],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Train severity classification model\n",
        "            print(\"Training severity classification model...\")\n",
        "            history_sev = self.classification_model.fit(\n",
        "                X_train, y_severity,\n",
        "                validation_data=(X_val, y_val_severity) if X_val is not None else None,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[early_stopping, reduce_lr] if X_val is not None else [],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Train pollutant classification model\n",
        "            print(\"Training pollutant classification model...\")\n",
        "            history_pol = self.severity_model.fit(\n",
        "                X_train, y_pollutant,\n",
        "                validation_data=(X_val, y_val_pollutant) if X_val is not None else None,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[early_stopping, reduce_lr] if X_val is not None else [],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            self.is_fitted = True\n",
        "            self.feature_cols = available_features\n",
        "            print(\"LSTM models trained successfully\")\n",
        "\n",
        "            return history_reg, history_sev, history_pol\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training LSTM models: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        Make predictions using fitted LSTM models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before making predictions\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df[self.feature_cols] = self.scaler.transform(test_df[self.feature_cols])\n",
        "\n",
        "            # Create sequences\n",
        "            X_test, _ = self.create_sequences(test_df, 'AQI', self.feature_cols)\n",
        "\n",
        "            if len(X_test) == 0:\n",
        "                raise ValueError(\"Not enough test data to create sequences\")\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred = self.regression_model.predict(X_test)\n",
        "            severity_pred = self.classification_model.predict(X_test)\n",
        "            pollutant_pred = self.severity_model.predict(X_test)\n",
        "\n",
        "            # Convert predictions to original format\n",
        "            aqi_pred = aqi_pred.flatten()\n",
        "            severity_pred = np.argmax(severity_pred, axis=1)\n",
        "            pollutant_pred = np.argmax(pollutant_pred, axis=1)\n",
        "\n",
        "            # Decode categorical predictions\n",
        "            severity_labels = self.severity_encoder.inverse_transform(severity_pred)\n",
        "            pollutant_labels = self.pollutant_encoder.inverse_transform(pollutant_pred)\n",
        "\n",
        "            return aqi_pred, severity_labels, pollutant_labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate model performance\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before evaluation\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df[self.feature_cols] = self.scaler.transform(test_df[self.feature_cols])\n",
        "\n",
        "            # Create sequences\n",
        "            X_test, y_aqi = self.create_sequences(test_df, 'AQI', self.feature_cols)\n",
        "            _, y_severity = self.create_sequences(test_df, 'Severity_encoded', self.feature_cols)\n",
        "            _, y_pollutant = self.create_sequences(test_df, 'Main_Pollutant_encoded', self.feature_cols)\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred, severity_pred, pollutant_pred = self.predict(test_data)\n",
        "\n",
        "            # Calculate metrics\n",
        "            aqi_rmse = np.sqrt(mean_squared_error(y_aqi, aqi_pred))\n",
        "            severity_acc = accuracy_score(y_severity,\n",
        "                                        self.severity_encoder.transform(severity_pred))\n",
        "            pollutant_acc = accuracy_score(y_pollutant,\n",
        "                                         self.pollutant_encoder.transform(pollutant_pred))\n",
        "\n",
        "            metrics = {\n",
        "                'AQI_RMSE': aqi_rmse,\n",
        "                'Severity_Accuracy': severity_acc,\n",
        "                'Pollutant_Accuracy': pollutant_acc\n",
        "            }\n",
        "\n",
        "            print(f\"LSTM Model Evaluation:\")\n",
        "            print(f\"AQI RMSE: {aqi_rmse:.4f}\")\n",
        "            print(f\"Severity Accuracy: {severity_acc:.4f}\")\n",
        "            print(f\"Pollutant Accuracy: {pollutant_acc:.4f}\")\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating models: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def save_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Save the fitted models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before saving\")\n",
        "\n",
        "        try:\n",
        "            # Save Keras models\n",
        "            self.regression_model.save(f\"{filepath_prefix}_regression.h5\")\n",
        "            self.classification_model.save(f\"{filepath_prefix}_severity.h5\")\n",
        "            self.severity_model.save(f\"{filepath_prefix}_pollutant.h5\")\n",
        "\n",
        "            # Save preprocessing objects\n",
        "            preprocessing_data = {\n",
        "                'scaler': self.scaler,\n",
        "                'severity_encoder': self.severity_encoder,\n",
        "                'pollutant_encoder': self.pollutant_encoder,\n",
        "                'feature_cols': self.feature_cols,\n",
        "                'sequence_length': self.sequence_length,\n",
        "                'lstm_units': self.lstm_units,\n",
        "                'dropout_rate': self.dropout_rate\n",
        "            }\n",
        "            joblib.dump(preprocessing_data, f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "\n",
        "            print(f\"LSTM models saved with prefix {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving models: {str(e)}\")\n",
        "\n",
        "    def load_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Load saved models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load Keras models\n",
        "            self.regression_model = tf.keras.models.load_model(f\"{filepath_prefix}_regression.h5\")\n",
        "            self.classification_model = tf.keras.models.load_model(f\"{filepath_prefix}_severity.h5\")\n",
        "            self.severity_model = tf.keras.models.load_model(f\"{filepath_prefix}_pollutant.h5\")\n",
        "\n",
        "            # Load preprocessing objects\n",
        "            preprocessing_data = joblib.load(f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "            self.scaler = preprocessing_data['scaler']\n",
        "            self.severity_encoder = preprocessing_data['severity_encoder']\n",
        "            self.pollutant_encoder = preprocessing_data['pollutant_encoder']\n",
        "            self.feature_cols = preprocessing_data['feature_cols']\n",
        "            self.sequence_length = preprocessing_data['sequence_length']\n",
        "            self.lstm_units = preprocessing_data['lstm_units']\n",
        "            self.dropout_rate = preprocessing_data['dropout_rate']\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(f\"LSTM models loaded from {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {str(e)}\")"
      ],
      "metadata": {
        "id": "Fhbf97oVzxrK"
      },
      "id": "Fhbf97oVzxrK",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForestModel:\n",
        "    def __init__(self, n_estimators=100, max_depth=None, random_state=42, n_jobs=-1):\n",
        "        \"\"\"\n",
        "        Initialize Random Forest models for AQI prediction\n",
        "\n",
        "        Parameters:\n",
        "        n_estimators: Number of trees in the forest\n",
        "        max_depth: Maximum depth of trees\n",
        "        random_state: Random state for reproducibility\n",
        "        n_jobs: Number of parallel jobs\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "        # Models\n",
        "        self.regression_model = None\n",
        "        self.severity_classifier = None\n",
        "        self.pollutant_classifier = None\n",
        "\n",
        "        # Preprocessing\n",
        "        self.scaler = StandardScaler()\n",
        "        self.severity_encoder = LabelEncoder()\n",
        "        self.pollutant_encoder = LabelEncoder()\n",
        "        self.feature_cols = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepare data for Random Forest modeling\n",
        "        \"\"\"\n",
        "        # Convert date and add time features\n",
        "        df['From Date'] = pd.to_datetime(df['From Date'])\n",
        "        df['Hour'] = df['From Date'].dt.hour\n",
        "        df['Day'] = df['From Date'].dt.day\n",
        "        df['Month'] = df['From Date'].dt.month\n",
        "        df['Year'] = df['From Date'].dt.year\n",
        "        df['DayOfWeek'] = df['From Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['From Date'].dt.quarter\n",
        "        df['DayOfYear'] = df['From Date'].dt.dayofyear\n",
        "\n",
        "        # Handle missing values\n",
        "        numeric_cols = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                        'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                        'WD (degree)', 'WS (m/s)']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        if not self.is_fitted:\n",
        "            df['Severity_encoded'] = self.severity_encoder.fit_transform(df['Severity'])\n",
        "            df['Main_Pollutant_encoded'] = self.pollutant_encoder.fit_transform(df['Main Pollutant'])\n",
        "        else:\n",
        "            df['Severity_encoded'] = self.severity_encoder.transform(df['Severity'])\n",
        "            df['Main_Pollutant_encoded'] = self.pollutant_encoder.transform(df['Main Pollutant'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_features(self, df):\n",
        "        \"\"\"\n",
        "        Create additional features for better prediction\n",
        "        \"\"\"\n",
        "        # Time-based features\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        df['IsRushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9) |\n",
        "                           (df['Hour'] >= 17) & (df['Hour'] <= 19)).astype(int)\n",
        "\n",
        "        # Seasonal features\n",
        "        df['Season'] = pd.cut(df['Month'], bins=[0, 3, 6, 9, 12],\n",
        "                             labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
        "        df['Season_encoded'] = pd.Categorical(df['Season']).codes\n",
        "\n",
        "        # Lagged features (if enough data)\n",
        "        if len(df) > 24:\n",
        "            df['AQI_lag1'] = df['AQI'].shift(1)\n",
        "            df['AQI_lag24'] = df['AQI'].shift(24)\n",
        "            df['AQI_rolling_mean_24'] = df['AQI'].rolling(window=24).mean()\n",
        "            df['AQI_rolling_std_24'] = df['AQI'].rolling(window=24).std()\n",
        "            df['AQI_rolling_max_24'] = df['AQI'].rolling(window=24).max()\n",
        "            df['AQI_rolling_min_24'] = df['AQI'].rolling(window=24).min()\n",
        "\n",
        "        # Pollutant ratios and interactions\n",
        "        if 'NO (ug/m3)' in df.columns and 'NOx (ug/m3)' in df.columns:\n",
        "            df['NO_NOx_ratio'] = df['NO (ug/m3)'] / (df['NOx (ug/m3)'] + 1e-8)\n",
        "\n",
        "        if 'Benzene (ug/m3)' in df.columns and 'Toluene (ug/m3)' in df.columns:\n",
        "            df['Benzene_Toluene_ratio'] = df['Benzene (ug/m3)'] / (df['Toluene (ug/m3)'] + 1e-8)\n",
        "\n",
        "        # Weather interactions\n",
        "        if 'AT (degree C)' in df.columns and 'RH (%)' in df.columns:\n",
        "            df['Temp_Humidity_interaction'] = df['AT (degree C)'] * df['RH (%)']\n",
        "\n",
        "        if 'WS (m/s)' in df.columns and 'WD (degree)' in df.columns:\n",
        "            df['Wind_component_x'] = df['WS (m/s)'] * np.cos(np.radians(df['WD (degree)']))\n",
        "            df['Wind_component_y'] = df['WS (m/s)'] * np.sin(np.radians(df['WD (degree)']))\n",
        "\n",
        "        # Solar radiation and temperature interaction\n",
        "        if 'SR (W/mt2)' in df.columns and 'AT (degree C)' in df.columns:\n",
        "            df['Solar_Temp_interaction'] = df['SR (W/mt2)'] * df['AT (degree C)']\n",
        "\n",
        "        # Air pressure and humidity interaction\n",
        "        if 'BP (mmHg)' in df.columns and 'RH (%)' in df.columns:\n",
        "            df['Pressure_Humidity_interaction'] = df['BP (mmHg)'] * df['RH (%)']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"\n",
        "        Fit Random Forest models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_df = self.prepare_data(train_data.copy())\n",
        "            train_df = self.create_features(train_df)\n",
        "\n",
        "            # Select features\n",
        "            base_features = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                            'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                            'WD (degree)', 'WS (m/s)', 'Hour', 'Day', 'Month', 'Year', 'DayOfWeek',\n",
        "                            'Quarter', 'DayOfYear', 'IsWeekend', 'IsRushHour', 'Season_encoded',\n",
        "                            'latitude', 'longitude', 'elevation']\n",
        "\n",
        "            # Add engineered features\n",
        "            engineered_features = ['AQI_lag1', 'AQI_lag24', 'AQI_rolling_mean_24', 'AQI_rolling_std_24',\n",
        "                                 'AQI_rolling_max_24', 'AQI_rolling_min_24', 'NO_NOx_ratio',\n",
        "                                 'Benzene_Toluene_ratio', 'Temp_Humidity_interaction', 'Wind_component_x',\n",
        "                                 'Wind_component_y', 'Solar_Temp_interaction', 'Pressure_Humidity_interaction']\n",
        "\n",
        "            all_features = base_features + engineered_features\n",
        "\n",
        "            # Filter available columns\n",
        "            self.feature_cols = [col for col in all_features if col in train_df.columns]\n",
        "\n",
        "            # Remove rows with NaN values (from lagged features)\n",
        "            train_df = train_df.dropna()\n",
        "\n",
        "            if len(train_df) == 0:\n",
        "                raise ValueError(\"No data remaining after removing NaN values\")\n",
        "\n",
        "            # Prepare features and targets\n",
        "            X_train = train_df[self.feature_cols]\n",
        "            y_aqi = train_df['AQI']\n",
        "            y_severity = train_df['Severity_encoded']\n",
        "            y_pollutant = train_df['Main_Pollutant_encoded']\n",
        "\n",
        "            # Scale features (Random Forest doesn't require scaling, but we'll keep it for consistency)\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "\n",
        "            # Initialize models\n",
        "            self.regression_model = RandomForestRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=self.n_jobs,\n",
        "                oob_score=True\n",
        "            )\n",
        "\n",
        "            self.severity_classifier = RandomForestClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=self.n_jobs,\n",
        "                oob_score=True\n",
        "            )\n",
        "\n",
        "            self.pollutant_classifier = RandomForestClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=self.n_jobs,\n",
        "                oob_score=True\n",
        "            )\n",
        "\n",
        "            # Fit models\n",
        "            print(\"Training AQI regression model...\")\n",
        "            self.regression_model.fit(X_train_scaled, y_aqi)\n",
        "            print(f\"OOB Score (R²): {self.regression_model.oob_score_:.4f}\")\n",
        "\n",
        "            print(\"Training severity classification model...\")\n",
        "            self.severity_classifier.fit(X_train_scaled, y_severity)\n",
        "            print(f\"OOB Score (Accuracy): {self.severity_classifier.oob_score_:.4f}\")\n",
        "\n",
        "            print(\"Training pollutant classification model...\")\n",
        "            self.pollutant_classifier.fit(X_train_scaled, y_pollutant)\n",
        "            print(f\"OOB Score (Accuracy): {self.pollutant_classifier.oob_score_:.4f}\")\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(\"Random Forest models trained successfully\")\n",
        "\n",
        "            # Feature importance\n",
        "            print(\"\\nTop 10 most important features for AQI prediction:\")\n",
        "            feature_importance = self.regression_model.feature_importances_\n",
        "            indices = np.argsort(feature_importance)[::-1][:10]\n",
        "            for i, idx in enumerate(indices):\n",
        "                print(f\"{i+1}. {self.feature_cols[idx]}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "            return self.regression_model, self.severity_classifier, self.pollutant_classifier\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training Random Forest models: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        Make predictions using fitted Random Forest models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before making predictions\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df = self.create_features(test_df)\n",
        "\n",
        "            # Handle missing lagged features for prediction\n",
        "            if 'AQI_lag1' in self.feature_cols and 'AQI_lag1' not in test_df.columns:\n",
        "                test_df['AQI_lag1'] = test_df['AQI'].shift(1).fillna(test_df['AQI'].mean())\n",
        "            if 'AQI_lag24' in self.feature_cols and 'AQI_lag24' not in test_df.columns:\n",
        "                test_df['AQI_lag24'] = test_df['AQI'].shift(24).fillna(test_df['AQI'].mean())\n",
        "\n",
        "            # Handle rolling features\n",
        "            rolling_features = ['AQI_rolling_mean_24', 'AQI_rolling_std_24', 'AQI_rolling_max_24', 'AQI_rolling_min_24']\n",
        "            for feature in rolling_features:\n",
        "                if feature in self.feature_cols and feature not in test_df.columns:\n",
        "                    if 'mean' in feature:\n",
        "                        test_df[feature] = test_df['AQI'].rolling(window=24).mean().fillna(test_df['AQI'].mean())\n",
        "                    elif 'std' in feature:\n",
        "                        test_df[feature] = test_df['AQI'].rolling(window=24).std().fillna(test_df['AQI'].std())\n",
        "                    elif 'max' in feature:\n",
        "                        test_df[feature] = test_df['AQI'].rolling(window=24).max().fillna(test_df['AQI'].max())\n",
        "                    elif 'min' in feature:\n",
        "                        test_df[feature] = test_df['AQI'].rolling(window=24).min().fillna(test_df['AQI'].min())\n",
        "\n",
        "            # Fill remaining NaN values\n",
        "            test_df = test_df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # Prepare features\n",
        "            X_test = test_df[self.feature_cols]\n",
        "            X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred = self.regression_model.predict(X_test_scaled)\n",
        "            severity_pred = self.severity_classifier.predict(X_test_scaled)\n",
        "            pollutant_pred = self.pollutant_classifier.predict(X_test_scaled)\n",
        "\n",
        "            # Get prediction probabilities for additional information\n",
        "            severity_proba = self.severity_classifier.predict_proba(X_test_scaled)\n",
        "            pollutant_proba = self.pollutant_classifier.predict_proba(X_test_scaled)\n",
        "\n",
        "            # Decode categorical predictions\n",
        "            severity_labels = self.severity_encoder.inverse_transform(severity_pred)\n",
        "            pollutant_labels = self.pollutant_encoder.inverse_transform(pollutant_pred)\n",
        "\n",
        "            return aqi_pred, severity_labels, pollutant_labels, severity_proba, pollutant_proba\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions: {str(e)}\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate model performance\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before evaluation\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df = self.create_features(test_df)\n",
        "            test_df = test_df.dropna()\n",
        "\n",
        "            if len(test_df) == 0:\n",
        "                raise ValueError(\"No test data remaining after removing NaN values\")\n",
        "\n",
        "            # Get actual values\n",
        "            y_true_aqi = test_df['AQI']\n",
        "            y_true_severity = test_df['Severity_encoded']\n",
        "            y_true_pollutant = test_df['Main_Pollutant_encoded']\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred, severity_pred, pollutant_pred, _, _ = self.predict(test_data)\n",
        "\n",
        "            # Calculate metrics\n",
        "            aqi_rmse = np.sqrt(mean_squared_error(y_true_aqi, aqi_pred[:len(y_true_aqi)]))\n",
        "            aqi_mae = np.mean(np.abs(y_true_aqi - aqi_pred[:len(y_true_aqi)]))\n",
        "\n",
        "            severity_acc = accuracy_score(y_true_severity,\n",
        "                                        self.severity_encoder.transform(severity_pred[:len(y_true_severity)]))\n",
        "            pollutant_acc = accuracy_score(y_true_pollutant,\n",
        "                                         self.pollutant_encoder.transform(pollutant_pred[:len(y_true_pollutant)]))\n",
        "\n",
        "            # F1 scores\n",
        "            severity_f1 = f1_score(y_true_severity,\n",
        "                                 self.severity_encoder.transform(severity_pred[:len(y_true_severity)]),\n",
        "                                 average='weighted')\n",
        "            pollutant_f1 = f1_score(y_true_pollutant,\n",
        "                                   self.pollutant_encoder.transform(pollutant_pred[:len(y_true_pollutant)]),\n",
        "                                   average='weighted')\n",
        "\n",
        "            # R² score\n",
        "            ss_res = np.sum((y_true_aqi - aqi_pred[:len(y_true_aqi)]) ** 2)\n",
        "            ss_tot = np.sum((y_true_aqi - np.mean(y_true_aqi)) ** 2)\n",
        "            r2_score = 1 - (ss_res / ss_tot)\n",
        "\n",
        "            metrics = {\n",
        "                'AQI_RMSE': aqi_rmse,\n",
        "                'AQI_MAE': aqi_mae,\n",
        "                'AQI_R2': r2_score,\n",
        "                'Severity_Accuracy': severity_acc,\n",
        "                'Severity_F1': severity_f1,\n",
        "                'Pollutant_Accuracy': pollutant_acc,\n",
        "                'Pollutant_F1': pollutant_f1\n",
        "            }\n",
        "\n",
        "            print(f\"Random Forest Model Evaluation:\")\n",
        "            print(f\"AQI RMSE: {aqi_rmse:.4f}\")\n",
        "            print(f\"AQI MAE: {aqi_mae:.4f}\")\n",
        "            print(f\"AQI R²: {r2_score:.4f}\")\n",
        "            print(f\"Severity Accuracy: {severity_acc:.4f}\")\n",
        "            print(f\"Severity F1: {severity_f1:.4f}\")\n",
        "            print(f\"Pollutant Accuracy: {pollutant_acc:.4f}\")\n",
        "            print(f\"Pollutant F1: {pollutant_f1:.4f}\")\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating models: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"\n",
        "        Get feature importance from the regression model\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before getting feature importance\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_cols,\n",
        "            'importance': self.regression_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return importance_df\n",
        "\n",
        "    def save_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Save the fitted models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before saving\")\n",
        "\n",
        "        try:\n",
        "            # Save models\n",
        "            joblib.dump(self.regression_model, f\"{filepath_prefix}_regression.pkl\")\n",
        "            joblib.dump(self.severity_classifier, f\"{filepath_prefix}_severity.pkl\")\n",
        "            joblib.dump(self.pollutant_classifier, f\"{filepath_prefix}_pollutant.pkl\")\n",
        "\n",
        "            # Save preprocessing objects\n",
        "            preprocessing_data = {\n",
        "                'scaler': self.scaler,\n",
        "                'severity_encoder': self.severity_encoder,\n",
        "                'pollutant_encoder': self.pollutant_encoder,\n",
        "                'feature_cols': self.feature_cols,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'max_depth': self.max_depth,\n",
        "                'random_state': self.random_state,\n",
        "                'n_jobs': self.n_jobs\n",
        "            }\n",
        "            joblib.dump(preprocessing_data, f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "\n",
        "            print(f\"Random Forest models saved with prefix {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving models: {str(e)}\")\n",
        "\n",
        "    def load_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Load saved models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load models\n",
        "            self.regression_model = joblib.load(f\"{filepath_prefix}_regression.pkl\")\n",
        "            self.severity_classifier = joblib.load(f\"{filepath_prefix}_severity.pkl\")\n",
        "            self.pollutant_classifier = joblib.load(f\"{filepath_prefix}_pollutant.pkl\")\n",
        "\n",
        "            # Load preprocessing objects\n",
        "            preprocessing_data = joblib.load(f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "            self.scaler = preprocessing_data['scaler']\n",
        "            self.severity_encoder = preprocessing_data['severity_encoder']\n",
        "            self.pollutant_encoder = preprocessing_data['pollutant_encoder']\n",
        "            self.feature_cols = preprocessing_data['feature_cols']\n",
        "            self.n_estimators = preprocessing_data['n_estimators']\n",
        "            self.max_depth = preprocessing_data['max_depth']\n",
        "            self.random_state = preprocessing_data['random_state']\n",
        "            self.n_jobs = preprocessing_data['n_jobs']\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(f\"Random Forest models loaded from {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {str(e)}\")"
      ],
      "metadata": {
        "id": "pTTiwXkRzzE4"
      },
      "id": "pTTiwXkRzzE4",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostModel:\n",
        "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize XGBoost models for AQI prediction\n",
        "\n",
        "        Parameters:\n",
        "        n_estimators: Number of boosting rounds\n",
        "        max_depth: Maximum depth of trees\n",
        "        learning_rate: Learning rate\n",
        "        random_state: Random state for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # Models\n",
        "        self.regression_model = None\n",
        "        self.severity_classifier = None\n",
        "        self.pollutant_classifier = None\n",
        "\n",
        "        # Preprocessing\n",
        "        self.scaler = StandardScaler()\n",
        "        self.severity_encoder = LabelEncoder()\n",
        "        self.pollutant_encoder = LabelEncoder()\n",
        "        self.feature_cols = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepare data for XGBoost modeling\n",
        "        \"\"\"\n",
        "        # Convert date and add time features\n",
        "        df['From Date'] = pd.to_datetime(df['From Date'])\n",
        "        df['Hour'] = df['From Date'].dt.hour\n",
        "        df['Day'] = df['From Date'].dt.day\n",
        "        df['Month'] = df['From Date'].dt.month\n",
        "        df['Year'] = df['From Date'].dt.year\n",
        "        df['DayOfWeek'] = df['From Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['From Date'].dt.quarter\n",
        "\n",
        "        # Handle missing values\n",
        "        numeric_cols = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                        'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                        'WD (degree)', 'WS (m/s)']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        if not self.is_fitted:\n",
        "            df['Severity_encoded'] = self.severity_encoder.fit_transform(df['Severity'])\n",
        "            df['Main_Pollutant_encoded'] = self.pollutant_encoder.fit_transform(df['Main Pollutant'])\n",
        "        else:\n",
        "            df['Severity_encoded'] = self.severity_encoder.transform(df['Severity'])\n",
        "            df['Main_Pollutant_encoded'] = self.pollutant_encoder.transform(df['Main Pollutant'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_features(self, df):\n",
        "        \"\"\"\n",
        "        Create additional features for better prediction\n",
        "        \"\"\"\n",
        "        # Lagged features (if enough data)\n",
        "        if len(df) > 24:\n",
        "            df['AQI_lag1'] = df['AQI'].shift(1)\n",
        "            df['AQI_lag24'] = df['AQI'].shift(24)\n",
        "            df['AQI_rolling_mean_24'] = df['AQI'].rolling(window=24).mean()\n",
        "            df['AQI_rolling_std_24'] = df['AQI'].rolling(window=24).std()\n",
        "\n",
        "        # Pollutant ratios\n",
        "        if 'NO (ug/m3)' in df.columns and 'NOx (ug/m3)' in df.columns:\n",
        "            df['NO_NOx_ratio'] = df['NO (ug/m3)'] / (df['NOx (ug/m3)'] + 1e-8)\n",
        "\n",
        "        # Weather interactions\n",
        "        if 'AT (degree C)' in df.columns and 'RH (%)' in df.columns:\n",
        "            df['Temp_Humidity_interaction'] = df['AT (degree C)'] * df['RH (%)']\n",
        "\n",
        "        if 'WS (m/s)' in df.columns and 'WD (degree)' in df.columns:\n",
        "            df['Wind_component_x'] = df['WS (m/s)'] * np.cos(np.radians(df['WD (degree)']))\n",
        "            df['Wind_component_y'] = df['WS (m/s)'] * np.sin(np.radians(df['WD (degree)']))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"\n",
        "        Fit XGBoost models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_df = self.prepare_data(train_data.copy())\n",
        "            train_df = self.create_features(train_df)\n",
        "\n",
        "            # Select features\n",
        "            base_features = ['AT (degree C)', 'BP (mmHg)', 'Benzene (ug/m3)', 'NO (ug/m3)',\n",
        "                            'NOx (ug/m3)', 'RF (mm)', 'RH (%)', 'SR (W/mt2)', 'Toluene (ug/m3)',\n",
        "                            'WD (degree)', 'WS (m/s)', 'Hour', 'Day', 'Month', 'Year', 'DayOfWeek',\n",
        "                            'Quarter', 'latitude', 'longitude', 'elevation']\n",
        "\n",
        "            # Add engineered features\n",
        "            engineered_features = ['AQI_lag1', 'AQI_lag24', 'AQI_rolling_mean_24', 'AQI_rolling_std_24',\n",
        "                                 'NO_NOx_ratio', 'Temp_Humidity_interaction', 'Wind_component_x', 'Wind_component_y']\n",
        "\n",
        "            all_features = base_features + engineered_features\n",
        "\n",
        "            # Filter available columns\n",
        "            self.feature_cols = [col for col in all_features if col in train_df.columns]\n",
        "\n",
        "            # Remove rows with NaN values (from lagged features)\n",
        "            train_df = train_df.dropna()\n",
        "\n",
        "            if len(train_df) == 0:\n",
        "                raise ValueError(\"No data remaining after removing NaN values\")\n",
        "\n",
        "            # Prepare features and targets\n",
        "            X_train = train_df[self.feature_cols]\n",
        "            y_aqi = train_df['AQI']\n",
        "            y_severity = train_df['Severity_encoded']\n",
        "            y_pollutant = train_df['Main_Pollutant_encoded']\n",
        "\n",
        "            # Scale features\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "\n",
        "            # Initialize models\n",
        "            self.regression_model = XGBRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                learning_rate=self.learning_rate,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            self.severity_classifier = XGBClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                learning_rate=self.learning_rate,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            self.pollutant_classifier = XGBClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                learning_rate=self.learning_rate,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # Fit models\n",
        "            print(\"Training AQI regression model...\")\n",
        "            self.regression_model.fit(X_train_scaled, y_aqi)\n",
        "\n",
        "            print(\"Training severity classification model...\")\n",
        "            self.severity_classifier.fit(X_train_scaled, y_severity)\n",
        "\n",
        "            print(\"Training pollutant classification model...\")\n",
        "            self.pollutant_classifier.fit(X_train_scaled, y_pollutant)\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(\"XGBoost models trained successfully\")\n",
        "\n",
        "            # Feature importance\n",
        "            print(\"\\nTop 10 most important features for AQI prediction:\")\n",
        "            feature_importance = self.regression_model.feature_importances_\n",
        "            indices = np.argsort(feature_importance)[::-1][:10]\n",
        "            for i, idx in enumerate(indices):\n",
        "                print(f\"{i+1}. {self.feature_cols[idx]}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "            return self.regression_model, self.severity_classifier, self.pollutant_classifier\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training XGBoost models: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        Make predictions using fitted XGBoost models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before making predictions\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df = self.create_features(test_df)\n",
        "\n",
        "            # Handle missing lagged features for prediction\n",
        "            if 'AQI_lag1' in self.feature_cols and 'AQI_lag1' not in test_df.columns:\n",
        "                test_df['AQI_lag1'] = test_df['AQI'].shift(1).fillna(test_df['AQI'].mean())\n",
        "            if 'AQI_lag24' in self.feature_cols and 'AQI_lag24' not in test_df.columns:\n",
        "                test_df['AQI_lag24'] = test_df['AQI'].shift(24).fillna(test_df['AQI'].mean())\n",
        "            if 'AQI_rolling_mean_24' in self.feature_cols and 'AQI_rolling_mean_24' not in test_df.columns:\n",
        "                test_df['AQI_rolling_mean_24'] = test_df['AQI'].rolling(window=24).mean().fillna(test_df['AQI'].mean())\n",
        "            if 'AQI_rolling_std_24' in self.feature_cols and 'AQI_rolling_std_24' not in test_df.columns:\n",
        "                test_df['AQI_rolling_std_24'] = test_df['AQI'].rolling(window=24).std().fillna(test_df['AQI'].std())\n",
        "\n",
        "            # Fill remaining NaN values\n",
        "            test_df = test_df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # Prepare features\n",
        "            X_test = test_df[self.feature_cols]\n",
        "            X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred = self.regression_model.predict(X_test_scaled)\n",
        "            severity_pred = self.severity_classifier.predict(X_test_scaled)\n",
        "            pollutant_pred = self.pollutant_classifier.predict(X_test_scaled)\n",
        "\n",
        "            # Decode categorical predictions\n",
        "            severity_labels = self.severity_encoder.inverse_transform(severity_pred)\n",
        "            pollutant_labels = self.pollutant_encoder.inverse_transform(pollutant_pred)\n",
        "\n",
        "            return aqi_pred, severity_labels, pollutant_labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate model performance\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before evaluation\")\n",
        "\n",
        "        try:\n",
        "            # Prepare test data\n",
        "            test_df = self.prepare_data(test_data.copy())\n",
        "            test_df = self.create_features(test_df)\n",
        "            test_df = test_df.dropna()\n",
        "\n",
        "            if len(test_df) == 0:\n",
        "                raise ValueError(\"No test data remaining after removing NaN values\")\n",
        "\n",
        "            # Get actual values\n",
        "            y_true_aqi = test_df['AQI']\n",
        "            y_true_severity = test_df['Severity_encoded']\n",
        "            y_true_pollutant = test_df['Main_Pollutant_encoded']\n",
        "\n",
        "            # Make predictions\n",
        "            aqi_pred, severity_pred, pollutant_pred = self.predict(test_data)\n",
        "\n",
        "            # Calculate metrics\n",
        "            aqi_rmse = np.sqrt(mean_squared_error(y_true_aqi, aqi_pred[:len(y_true_aqi)]))\n",
        "            severity_acc = accuracy_score(y_true_severity,\n",
        "                                        self.severity_encoder.transform(severity_pred[:len(y_true_severity)]))\n",
        "            pollutant_acc = accuracy_score(y_true_pollutant,\n",
        "                                         self.pollutant_encoder.transform(pollutant_pred[:len(y_true_pollutant)]))\n",
        "\n",
        "            # F1 scores\n",
        "            severity_f1 = f1_score(y_true_severity,\n",
        "                                 self.severity_encoder.transform(severity_pred[:len(y_true_severity)]),\n",
        "                                 average='weighted')\n",
        "            pollutant_f1 = f1_score(y_true_pollutant,\n",
        "                                   self.pollutant_encoder.transform(pollutant_pred[:len(y_true_pollutant)]),\n",
        "                                   average='weighted')\n",
        "\n",
        "            metrics = {\n",
        "                'AQI_RMSE': aqi_rmse,\n",
        "                'Severity_Accuracy': severity_acc,\n",
        "                'Severity_F1': severity_f1,\n",
        "                'Pollutant_Accuracy': pollutant_acc,\n",
        "                'Pollutant_F1': pollutant_f1\n",
        "            }\n",
        "\n",
        "            print(f\"XGBoost Model Evaluation:\")\n",
        "            print(f\"AQI RMSE: {aqi_rmse:.4f}\")\n",
        "            print(f\"Severity Accuracy: {severity_acc:.4f}\")\n",
        "            print(f\"Severity F1: {severity_f1:.4f}\")\n",
        "            print(f\"Pollutant Accuracy: {pollutant_acc:.4f}\")\n",
        "            print(f\"Pollutant F1: {pollutant_f1:.4f}\")\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating models: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def save_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Save the fitted models\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before saving\")\n",
        "\n",
        "        try:\n",
        "            # Save models\n",
        "            joblib.dump(self.regression_model, f\"{filepath_prefix}_regression.pkl\")\n",
        "            joblib.dump(self.severity_classifier, f\"{filepath_prefix}_severity.pkl\")\n",
        "            joblib.dump(self.pollutant_classifier, f\"{filepath_prefix}_pollutant.pkl\")\n",
        "\n",
        "            # Save preprocessing objects\n",
        "            preprocessing_data = {\n",
        "                'scaler': self.scaler,\n",
        "                'severity_encoder': self.severity_encoder,\n",
        "                'pollutant_encoder': self.pollutant_encoder,\n",
        "                'feature_cols': self.feature_cols,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'max_depth': self.max_depth,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'random_state': self.random_state\n",
        "            }\n",
        "            joblib.dump(preprocessing_data, f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "\n",
        "            print(f\"XGBoost models saved with prefix {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving models: {str(e)}\")\n",
        "\n",
        "    def load_model(self, filepath_prefix):\n",
        "        \"\"\"\n",
        "        Load saved models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load models\n",
        "            self.regression_model = joblib.load(f\"{filepath_prefix}_regression.pkl\")\n",
        "            self.severity_classifier = joblib.load(f\"{filepath_prefix}_severity.pkl\")\n",
        "            self.pollutant_classifier = joblib.load(f\"{filepath_prefix}_pollutant.pkl\")\n",
        "\n",
        "            # Load preprocessing objects\n",
        "            preprocessing_data = joblib.load(f\"{filepath_prefix}_preprocessing.pkl\")\n",
        "            self.scaler = preprocessing_data['scaler']\n",
        "            self.severity_encoder = preprocessing_data['severity_encoder']\n",
        "            self.pollutant_encoder = preprocessing_data['pollutant_encoder']\n",
        "            self.feature_cols = preprocessing_data['feature_cols']\n",
        "            self.n_estimators = preprocessing_data['n_estimators']\n",
        "            self.max_depth = preprocessing_data['max_depth']\n",
        "            self.learning_rate = preprocessing_data['learning_rate']\n",
        "            self.random_state = preprocessing_data['random_state']\n",
        "\n",
        "            self.is_fitted = True\n",
        "            print(f\"XGBoost models loaded from {filepath_prefix}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {str(e)}\")"
      ],
      "metadata": {
        "id": "fLLl1efaz3Y0"
      },
      "id": "fLLl1efaz3Y0",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6vTb6fJ0acX"
      },
      "id": "w6vTb6fJ0acX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}