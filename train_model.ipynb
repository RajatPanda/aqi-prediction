{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install opencage\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "TAnWF-3p1x2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb17824-7b05-43e4-937a-ec3e86892f00",
        "collapsed": true
      },
      "id": "TAnWF-3p1x2_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: opencage in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: Requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from opencage) (2.32.3)\n",
            "Requirement already satisfied: backoff>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from opencage) (2.2.1)\n",
            "Requirement already satisfied: tqdm>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from opencage) (4.67.1)\n",
            "Requirement already satisfied: certifi>=2024.07.04 in /usr/local/lib/python3.11/dist-packages (from opencage) (2025.6.15)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.11/dist-packages (from opencage) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.5->opencage) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from Requests>=2.31.0->opencage) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from geopy.geocoders import Nominatim\n",
        "from tqdm.notebook import tqdm\n",
        "from opencage.geocoder import OpenCageGeocode\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import warnings\n",
        "# Arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# LSTM\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# Random forest\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "KAmjdWX6zJSl"
      },
      "id": "KAmjdWX6zJSl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u51nzga5xoJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68ed8a7-892c-411c-8ac3-52509e5fedb9"
      },
      "id": "u51nzga5xoJg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ARIMAModel:\n",
        "    \"\"\"\n",
        "    Enhanced ARIMA Model with progress bar support for Jupyter notebooks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, order=(1, 1, 1)):\n",
        "        \"\"\"\n",
        "        Initialize ARIMA model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        order : tuple\n",
        "            (p, d, q) order for ARIMA model\n",
        "        \"\"\"\n",
        "        self.order = order\n",
        "        self.model = None\n",
        "        self.fitted_model = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _check_stationarity(self, timeseries, progress_bar=None):\n",
        "        \"\"\"Check if time series is stationary\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Checking stationarity...\")\n",
        "            time.sleep(0.1)  # Small delay for visual feedback\n",
        "\n",
        "        result = adfuller(timeseries.dropna())\n",
        "        p_value = result[1]\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\"ADF p-value\": f\"{p_value:.6f}\"})\n",
        "\n",
        "        return p_value < 0.05\n",
        "\n",
        "    def _prepare_data(self, data, progress_bar=None):\n",
        "        \"\"\"Prepare data for ARIMA modeling\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Preparing data...\")\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Handle missing values\n",
        "        if isinstance(data, pd.Series):\n",
        "            data = data.interpolate(method='time').fillna(method='bfill').fillna(method='ffill')\n",
        "        else:\n",
        "            data = pd.Series(data).interpolate().fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\"Data points\": len(data)})\n",
        "\n",
        "        return data\n",
        "\n",
        "    def fit(self, data, show_progress=True):\n",
        "        \"\"\"\n",
        "        Fit ARIMA model with progress tracking\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : array-like or pd.Series\n",
        "            Time series data\n",
        "        show_progress : bool\n",
        "            Whether to show progress bar\n",
        "        \"\"\"\n",
        "        # Initialize progress bar\n",
        "        progress_steps = 4\n",
        "        if show_progress:\n",
        "            pbar = tqdm(total=progress_steps, desc=\"Training ARIMA\",\n",
        "                       bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Prepare data\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            prepared_data = self._prepare_data(data, pbar)\n",
        "\n",
        "            # Step 2: Check stationarity\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            is_stationary = self._check_stationarity(prepared_data, pbar)\n",
        "\n",
        "            # Step 3: Fit model\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Fitting ARIMA model...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                self.model = ARIMA(prepared_data, order=self.order)\n",
        "                self.fitted_model = self.model.fit()\n",
        "\n",
        "            # Step 4: Finalize\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Model fitting complete\")\n",
        "                pbar.set_postfix({\n",
        "                    \"AIC\": f\"{self.fitted_model.aic:.2f}\",\n",
        "                    \"Order\": str(self.order)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "                time.sleep(0.5)  # Brief pause to show completion\n",
        "\n",
        "            self.is_fitted = True\n",
        "\n",
        "            if pbar:\n",
        "                pbar.close()\n",
        "\n",
        "            print(f\"âœ“ ARIMA{self.order} model fitted successfully!\")\n",
        "            print(f\"  AIC: {self.fitted_model.aic:.2f}\")\n",
        "            print(f\"  Data points: {len(prepared_data)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if pbar:\n",
        "                pbar.close()\n",
        "            print(f\"âœ— Error fitting ARIMA model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, steps=1):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        forecast = self.fitted_model.forecast(steps=steps)\n",
        "        return forecast\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the fitted model\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before saving\")\n",
        "\n",
        "        model_data = {\n",
        "            'fitted_model': self.fitted_model,\n",
        "            'order': self.order,\n",
        "            'is_fitted': self.is_fitted\n",
        "        }\n",
        "        joblib.dump(model_data, filepath)\n",
        "        print(f\"âœ“ ARIMA model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load a fitted model\"\"\"\n",
        "        model_data = joblib.load(filepath)\n",
        "        self.fitted_model = model_data['fitted_model']\n",
        "        self.order = model_data['order']\n",
        "        self.is_fitted = model_data['is_fitted']\n",
        "        print(f\"âœ“ ARIMA model loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "h0WoPUPrEee8"
      },
      "id": "h0WoPUPrEee8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel:\n",
        "    \"\"\"\n",
        "    Enhanced LSTM Model with GPU acceleration and progress bar support for Google Colab\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length=24, lstm_units=50, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Initialize LSTM model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        sequence_length : int\n",
        "            Number of time steps to look back\n",
        "        lstm_units : int\n",
        "            Number of LSTM units\n",
        "        dropout_rate : float\n",
        "            Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.model = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.is_fitted = False\n",
        "\n",
        "        # Setup GPU acceleration for Google Colab\n",
        "        self._setup_gpu()\n",
        "\n",
        "    def _setup_gpu(self):\n",
        "        \"\"\"Setup GPU acceleration for Google Colab\"\"\"\n",
        "        print(\"ðŸ”§ Setting up GPU acceleration for Google Colab...\")\n",
        "\n",
        "        # Check GPU availability\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                # Enable memory growth to prevent allocation of all GPU memory\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "                # Set up mixed precision for better performance\n",
        "                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "                tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "                print(f\"âœ“ GPU acceleration enabled!\")\n",
        "                print(f\"  Available GPUs: {len(gpus)}\")\n",
        "                print(f\"  GPU Names: {[gpu.name for gpu in gpus]}\")\n",
        "                print(f\"  Mixed precision: {policy.name}\")\n",
        "\n",
        "                # Display GPU memory info\n",
        "                gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
        "                print(f\"  GPU Details: {gpu_details.get('device_name', 'Unknown')}\")\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"âš ï¸  GPU setup warning: {e}\")\n",
        "        else:\n",
        "            print(\"âš ï¸  No GPU detected. Running on CPU.\")\n",
        "            print(\"   To enable GPU in Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "\n",
        "    def _create_sequences(self, data, target_col=None, progress_bar=None):\n",
        "        \"\"\"Create sequences for LSTM training\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Creating sequences...\")\n",
        "\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in tqdm(range(self.sequence_length, len(data)),\n",
        "                     desc=\"Building sequences\",\n",
        "                     disable=not progress_bar,\n",
        "                     leave=False):\n",
        "            X.append(data[i-self.sequence_length:i])\n",
        "            if target_col is not None:\n",
        "                y.append(data[i, target_col] if len(data.shape) > 1 else data[i])\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\"Sequences\": len(X)})\n",
        "\n",
        "        return np.array(X), np.array(y) if y else None\n",
        "\n",
        "    def _build_model(self, input_shape, output_shapes, progress_bar=None):\n",
        "        \"\"\"Build LSTM architecture\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Building LSTM architecture...\")\n",
        "\n",
        "        # Use GPU-optimized layers\n",
        "        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
        "            # Input layer\n",
        "            inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "            # LSTM layers (GPU-accelerated when conditions are met)\n",
        "            x = tf.keras.layers.LSTM(\n",
        "                self.lstm_units,\n",
        "                return_sequences=True,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=0,  # Required for GPU acceleration\n",
        "                activation='tanh',    # Required for GPU acceleration\n",
        "                recurrent_activation='sigmoid'  # Required for GPU acceleration\n",
        "            )(inputs)\n",
        "\n",
        "            x = tf.keras.layers.LSTM(\n",
        "                self.lstm_units,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=0,\n",
        "                activation='tanh',\n",
        "                recurrent_activation='sigmoid'\n",
        "            )(x)\n",
        "\n",
        "            # Output layers\n",
        "            outputs = {}\n",
        "\n",
        "            # AQI prediction (regression)\n",
        "            aqi_output = tf.keras.layers.Dense(1, name='aqi_output', dtype='float32')(x)\n",
        "            outputs['aqi'] = aqi_output\n",
        "\n",
        "            # Severity prediction (classification)\n",
        "            severity_output = tf.keras.layers.Dense(\n",
        "                output_shapes['severity'],\n",
        "                activation='softmax',\n",
        "                name='severity_output',\n",
        "                dtype='float32'\n",
        "            )(x)\n",
        "            outputs['severity'] = severity_output\n",
        "\n",
        "            # Main pollutant prediction (classification)\n",
        "            pollutant_output = tf.keras.layers.Dense(\n",
        "                output_shapes['pollutant'],\n",
        "                activation='softmax',\n",
        "                name='pollutant_output',\n",
        "                dtype='float32'\n",
        "            )(x)\n",
        "            outputs['pollutant'] = pollutant_output\n",
        "\n",
        "            # Create model\n",
        "            model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            # Compile with mixed precision optimizer\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "            if tf.config.list_physical_devices('GPU'):\n",
        "                optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss={\n",
        "                    'aqi': 'mse',\n",
        "                    'severity': 'sparse_categorical_crossentropy',\n",
        "                    'pollutant': 'sparse_categorical_crossentropy'\n",
        "                },\n",
        "                metrics={\n",
        "                    'aqi': ['mae'],\n",
        "                    'severity': ['accuracy'],\n",
        "                    'pollutant': ['accuracy']\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\n",
        "                \"Parameters\": f\"{model.count_params():,}\",\n",
        "                \"GPU\": \"Yes\" if tf.config.list_physical_devices('GPU') else \"No\"\n",
        "            })\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _prepare_data(self, data, progress_bar=None):\n",
        "        \"\"\"Prepare data for LSTM training\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Preparing data...\")\n",
        "\n",
        "        # Handle missing values\n",
        "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "        data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        categorical_columns = ['Severity', 'Main Pollutant']\n",
        "        for col in categorical_columns:\n",
        "            if col in data.columns:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    data[col] = self.label_encoders[col].fit_transform(data[col].astype(str))\n",
        "                else:\n",
        "                    data[col] = self.label_encoders[col].transform(data[col].astype(str))\n",
        "\n",
        "        # Scale numerical features\n",
        "        feature_columns = [col for col in data.columns if col not in ['Severity', 'Main Pollutant']]\n",
        "        scaled_features = self.scaler.fit_transform(data[feature_columns])\n",
        "\n",
        "        # Combine scaled features with encoded categorical variables\n",
        "        processed_data = np.column_stack([\n",
        "            scaled_features,\n",
        "            data['Severity'].values if 'Severity' in data.columns else np.zeros(len(data)),\n",
        "            data['Main Pollutant'].values if 'Main Pollutant' in data.columns else np.zeros(len(data))\n",
        "        ])\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\n",
        "                \"Features\": processed_data.shape[1],\n",
        "                \"Samples\": processed_data.shape[0]\n",
        "            })\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def fit(self, data, epochs=50, batch_size=32, validation_split=0.2, show_progress=True):\n",
        "        \"\"\"\n",
        "        Fit LSTM model with GPU acceleration and progress tracking\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pd.DataFrame\n",
        "            Training data\n",
        "        epochs : int\n",
        "            Number of training epochs\n",
        "        batch_size : int\n",
        "            Batch size for training\n",
        "        validation_split : float\n",
        "            Fraction of data to use for validation\n",
        "        show_progress : bool\n",
        "            Whether to show progress bar\n",
        "        \"\"\"\n",
        "        # Initialize progress bar\n",
        "        progress_steps = 5\n",
        "        if show_progress:\n",
        "            pbar = tqdm(total=progress_steps, desc=\"Training LSTM\",\n",
        "                       bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Prepare data\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            processed_data = self._prepare_data(data, pbar)\n",
        "\n",
        "            # Step 2: Create sequences\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            X, _ = self._create_sequences(processed_data, progress_bar=pbar)\n",
        "\n",
        "            # Prepare targets\n",
        "            y_aqi = data['AQI'].values[self.sequence_length:]\n",
        "            y_severity = data['Severity'].map(self.label_encoders['Severity'].transform) if 'Severity' in data.columns else np.zeros(len(y_aqi))\n",
        "            y_pollutant = data['Main Pollutant'].map(self.label_encoders['Main Pollutant'].transform) if 'Main Pollutant' in data.columns else np.zeros(len(y_aqi))\n",
        "\n",
        "            # Step 3: Build model\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            output_shapes = {\n",
        "                'severity': len(self.label_encoders.get('Severity', {}).classes_) if 'Severity' in self.label_encoders else 3,\n",
        "                'pollutant': len(self.label_encoders.get('Main Pollutant', {}).classes_) if 'Main Pollutant' in self.label_encoders else 5\n",
        "            }\n",
        "\n",
        "            self.model = self._build_model(\n",
        "                input_shape=(self.sequence_length, processed_data.shape[1]),\n",
        "                output_shapes=output_shapes,\n",
        "                progress_bar=pbar\n",
        "            )\n",
        "\n",
        "            # Step 4: Setup training callbacks\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Setting up training...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    patience=10,\n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                    monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=5\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            # Add TensorBoard callback for GPU monitoring\n",
        "            if tf.config.list_physical_devices('GPU'):\n",
        "                callbacks.append(\n",
        "                    tf.keras.callbacks.TensorBoard(\n",
        "                        log_dir='./logs',\n",
        "                        histogram_freq=1,\n",
        "                        profile_batch='2,5'\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Step 5: Train model\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Training on GPU...\" if tf.config.list_physical_devices('GPU') else \"Training on CPU...\")\n",
        "                pbar.update(1)\n",
        "                pbar.close()\n",
        "\n",
        "            # Prepare targets dictionary\n",
        "            y_dict = {\n",
        "                'aqi': y_aqi,\n",
        "                'severity': y_severity,\n",
        "                'pollutant': y_pollutant\n",
        "            }\n",
        "\n",
        "            print(f\"ðŸš€ Starting LSTM training on {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}...\")\n",
        "            print(f\"   Training samples: {len(X)}\")\n",
        "            print(f\"   Sequence length: {self.sequence_length}\")\n",
        "            print(f\"   Batch size: {batch_size}\")\n",
        "            print(f\"   Epochs: {epochs}\")\n",
        "\n",
        "            # Train with progress monitoring\n",
        "            history = self.model.fit(\n",
        "                X, y_dict,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1  # Show training progress\n",
        "            )\n",
        "\n",
        "            self.is_fitted = True\n",
        "\n",
        "            print(\"âœ“ LSTM model training completed!\")\n",
        "            print(f\"  Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "            print(f\"  Final val_loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "            return history\n",
        "\n",
        "        except Exception as e:\n",
        "            if pbar:\n",
        "                pbar.close()\n",
        "            print(f\"âœ— Error training LSTM model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        processed_data = self._prepare_data(data)\n",
        "        X, _ = self._create_sequences(processed_data)\n",
        "\n",
        "        predictions = self.model.predict(X)\n",
        "\n",
        "        return {\n",
        "            'aqi': predictions['aqi'].flatten(),\n",
        "            'severity': np.argmax(predictions['severity'], axis=1),\n",
        "            'pollutant': np.argmax(predictions['pollutant'], axis=1)\n",
        "        }\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the fitted model\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before saving\")\n",
        "\n",
        "        self.model.save(filepath)\n",
        "        print(f\"âœ“ LSTM model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load a fitted model\"\"\"\n",
        "        self.model = tf.keras.models.load_model(filepath)\n",
        "        self.is_fitted = True\n",
        "        print(f\"âœ“ LSTM model loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "Fhbf97oVzxrK"
      },
      "id": "Fhbf97oVzxrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForestModel:\n",
        "    \"\"\"\n",
        "    Enhanced Random Forest Model with progress bar support and GPU acceleration options\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_depth=None, random_state=42, n_jobs=-1, use_gpu_alternative=True):\n",
        "        \"\"\"\n",
        "        Initialize Random Forest model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_estimators : int\n",
        "            Number of trees in the forest\n",
        "        max_depth : int or None\n",
        "            Maximum depth of trees\n",
        "        random_state : int\n",
        "            Random state for reproducibility\n",
        "        n_jobs : int\n",
        "            Number of parallel jobs (-1 uses all cores)\n",
        "        use_gpu_alternative : bool\n",
        "            Whether to try GPU-accelerated alternatives if available\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.use_gpu_alternative = use_gpu_alternative\n",
        "\n",
        "        self.regressor = None\n",
        "        self.severity_classifier = None\n",
        "        self.pollutant_classifier = None\n",
        "        self.label_encoders = {}\n",
        "        self.feature_columns = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "        # Setup GPU alternatives\n",
        "        self._setup_gpu_alternatives()\n",
        "\n",
        "    def _setup_gpu_alternatives(self):\n",
        "        \"\"\"Setup GPU-accelerated alternatives for Random Forest\"\"\"\n",
        "        print(\"ðŸ”§ Setting up Random Forest with acceleration options...\")\n",
        "\n",
        "        if self.use_gpu_alternative:\n",
        "            try:\n",
        "                # Try to import cuML for GPU acceleration\n",
        "                import cuml\n",
        "                from cuml.ensemble import RandomForestRegressor as cuRFRegressor\n",
        "                from cuml.ensemble import RandomForestClassifier as cuRFClassifier\n",
        "\n",
        "                self.cuml_available = True\n",
        "                self.cuRFRegressor = cuRFRegressor\n",
        "                self.cuRFClassifier = cuRFClassifier\n",
        "\n",
        "                print(\"âœ“ cuML (GPU-accelerated Random Forest) is available!\")\n",
        "                print(\"  Will use GPU acceleration when possible\")\n",
        "\n",
        "                # Check GPU availability\n",
        "                import subprocess\n",
        "                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "                if result.returncode == 0:\n",
        "                    print(\"âœ“ NVIDIA GPU detected for cuML acceleration\")\n",
        "                else:\n",
        "                    print(\"âš ï¸  No NVIDIA GPU detected. cuML will use CPU.\")\n",
        "\n",
        "            except ImportError:\n",
        "                self.cuml_available = False\n",
        "                print(\"âš ï¸  cuML not available. Install with: pip install cuml-cu11\")\n",
        "                print(\"   Using standard scikit-learn Random Forest\")\n",
        "\n",
        "        else:\n",
        "            self.cuml_available = False\n",
        "            print(\"ðŸ–¥ï¸  Using standard scikit-learn Random Forest\")\n",
        "\n",
        "        # Setup multi-threading optimization\n",
        "        if self.n_jobs == -1:\n",
        "            n_cores = os.cpu_count()\n",
        "            print(f\"ðŸ“Š Using all {n_cores} CPU cores for parallel processing\")\n",
        "        else:\n",
        "            print(f\"ðŸ“Š Using {self.n_jobs} CPU cores for parallel processing\")\n",
        "\n",
        "    def _create_advanced_features(self, data, progress_bar=None):\n",
        "        \"\"\"Create advanced engineered features\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Creating advanced features...\")\n",
        "\n",
        "        feature_data = data.copy()\n",
        "\n",
        "        # Time-based features\n",
        "        if 'From Date' in data.columns:\n",
        "            feature_data['From Date'] = pd.to_datetime(feature_data['From Date'])\n",
        "            feature_data['hour'] = feature_data['From Date'].dt.hour\n",
        "            feature_data['day_of_week'] = feature_data['From Date'].dt.dayofweek\n",
        "            feature_data['month'] = feature_data['From Date'].dt.month\n",
        "            feature_data['season'] = (feature_data['From Date'].dt.month % 12 + 3) // 3\n",
        "            feature_data['is_weekend'] = (feature_data['day_of_week'] >= 5).astype(int)\n",
        "            feature_data['is_rush_hour'] = ((feature_data['hour'].between(7, 9)) |\n",
        "                                          (feature_data['hour'].between(17, 19))).astype(int)\n",
        "\n",
        "        # Weather interaction features\n",
        "        weather_cols = ['AT (degree C)', 'RH (%)', 'WS (m/s)', 'WD (degree)', 'SR (W/mt2)', 'RF (mm)', 'BP (mmHg)']\n",
        "        available_weather = [col for col in weather_cols if col in data.columns]\n",
        "\n",
        "        for i, col1 in enumerate(available_weather):\n",
        "            for col2 in available_weather[i+1:]:\n",
        "                if all(col in data.columns for col in [col1, col2]):\n",
        "                    interaction_name = f\"{col1.split()[0]}_{col2.split()[0]}_interaction\"\n",
        "                    feature_data[interaction_name] = feature_data[col1] * feature_data[col2]\n",
        "\n",
        "        # Wind components (if wind speed and direction available)\n",
        "        if all(col in data.columns for col in ['WS (m/s)', 'WD (degree)']):\n",
        "            feature_data['wind_x'] = feature_data['WS (m/s)'] * np.cos(np.radians(feature_data['WD (degree)']))\n",
        "            feature_data['wind_y'] = feature_data['WS (m/s)'] * np.sin(np.radians(feature_data['WD (degree)']))\n",
        "\n",
        "        # Pollutant features\n",
        "        pollutant_cols = ['Benzene (ug/m3)', 'NO (ug/m3)', 'NOx (ug/m3)', 'Toluene (ug/m3)']\n",
        "        available_pollutants = [col for col in pollutant_cols if col in data.columns]\n",
        "\n",
        "        if available_pollutants:\n",
        "            # Pollutant ratios\n",
        "            for i, col1 in enumerate(available_pollutants):\n",
        "                for col2 in available_pollutants[i+1:]:\n",
        "                    ratio_name = f\"{col1.split()[0]}_{col2.split()[0]}_ratio\"\n",
        "                    feature_data[ratio_name] = (feature_data[col1] + 1e-8) / (feature_data[col2] + 1e-8)\n",
        "\n",
        "            # Total pollutant load\n",
        "            feature_data['total_pollutants'] = feature_data[available_pollutants].sum(axis=1)\n",
        "\n",
        "        # Rolling statistics (if data is temporal)\n",
        "        if len(feature_data) > 24:  # At least 24 data points for meaningful rolling stats\n",
        "            numeric_cols = feature_data.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols[:5]:  # Limit to avoid too many features\n",
        "                if col in feature_data.columns:\n",
        "                    feature_data[f'{col}_rolling_mean_6h'] = feature_data[col].rolling(window=6, min_periods=1).mean()\n",
        "                    feature_data[f'{col}_rolling_std_6h'] = feature_data[col].rolling(window=6, min_periods=1).std()\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\"Total features\": feature_data.shape[1]})\n",
        "\n",
        "        return feature_data\n",
        "\n",
        "    def _prepare_data(self, data, progress_bar=None):\n",
        "        \"\"\"Prepare data for Random Forest training\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Preparing data for Random Forest...\")\n",
        "\n",
        "        # Create advanced features\n",
        "        feature_data = self._create_advanced_features(data, progress_bar)\n",
        "\n",
        "        # Handle missing values with different strategies\n",
        "        numeric_columns = feature_data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        # For weather data, use median imputation\n",
        "        weather_numeric = [col for col in numeric_columns if any(w in col for w in ['AT', 'RH', 'WS', 'WD', 'SR', 'RF', 'BP'])]\n",
        "        if weather_numeric:\n",
        "            feature_data[weather_numeric] = feature_data[weather_numeric].fillna(feature_data[weather_numeric].median())\n",
        "\n",
        "        # For other numeric data, use forward fill then median\n",
        "        other_numeric = [col for col in numeric_columns if col not in weather_numeric]\n",
        "        if other_numeric:\n",
        "            feature_data[other_numeric] = feature_data[other_numeric].fillna(method='ffill').fillna(feature_data[other_numeric].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        categorical_columns = ['Severity', 'Main Pollutant']\n",
        "        for col in categorical_columns:\n",
        "            if col in feature_data.columns:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    feature_data[col] = self.label_encoders[col].fit_transform(feature_data[col].astype(str))\n",
        "                else:\n",
        "                    feature_data[col] = self.label_encoders[col].transform(feature_data[col].astype(str))\n",
        "\n",
        "        # Select feature columns\n",
        "        exclude_cols = ['From Date', 'file_name', 'state', 'city']\n",
        "        self.feature_columns = [col for col in feature_data.columns\n",
        "                               if col not in exclude_cols and feature_data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\n",
        "                \"Features\": len(self.feature_columns),\n",
        "                \"Samples\": len(feature_data)\n",
        "            })\n",
        "\n",
        "        return feature_data[self.feature_columns]\n",
        "\n",
        "    def _train_with_progress(self, model, X_train, y_train, X_val, y_val, model_name, progress_bar=None):\n",
        "        \"\"\"Train model with progress tracking\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(f\"Training {model_name}...\")\n",
        "\n",
        "        print(f\"ðŸŒ³ Training {model_name} with {self.n_estimators} trees...\")\n",
        "\n",
        "        # For large datasets, show progress by training in batches\n",
        "        if self.cuml_available and hasattr(model, 'fit'):\n",
        "            # cuML models\n",
        "            start_time = time.time()\n",
        "            model.fit(X_train, y_train)\n",
        "            training_time = time.time() - start_time\n",
        "            print(f\"   Training completed in {training_time:.2f} seconds (GPU accelerated)\")\n",
        "        else:\n",
        "            # Scikit-learn models with warm start for progress tracking\n",
        "            if hasattr(model, 'warm_start'):\n",
        "                model.warm_start = True\n",
        "\n",
        "                # Train incrementally to show progress\n",
        "                batch_size = max(10, self.n_estimators // 10)\n",
        "                for i in range(batch_size, self.n_estimators + 1, batch_size):\n",
        "                    model.n_estimators = min(i, self.n_estimators)\n",
        "                    start_time = time.time()\n",
        "                    model.fit(X_train, y_train)\n",
        "                    batch_time = time.time() - start_time\n",
        "\n",
        "                    # Calculate validation score\n",
        "                    val_pred = model.predict(X_val)\n",
        "                    if hasattr(model, 'predict_proba'):  # Classifier\n",
        "                        score = accuracy_score(y_val, val_pred)\n",
        "                        metric = \"Accuracy\"\n",
        "                    else:  # Regressor\n",
        "                        score = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "                        metric = \"RMSE\"\n",
        "\n",
        "                    print(f\"   Trees {model.n_estimators:3d}/{self.n_estimators}: {metric} = {score:.4f} ({batch_time:.2f}s)\")\n",
        "\n",
        "                model.warm_start = False\n",
        "            else:\n",
        "                # Regular training\n",
        "                start_time = time.time()\n",
        "                model.fit(X_train, y_train)\n",
        "                training_time = time.time() - start_time\n",
        "                print(f\"   Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, data, show_progress=True):\n",
        "        \"\"\"\n",
        "        Fit Random Forest models with progress tracking and optional GPU acceleration\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pd.DataFrame\n",
        "            Training data\n",
        "        show_progress : bool\n",
        "            Whether to show progress bar\n",
        "        \"\"\"\n",
        "        # Initialize progress bar\n",
        "        progress_steps = 6\n",
        "        if show_progress:\n",
        "            pbar = tqdm(total=progress_steps, desc=\"Training Random Forest\",\n",
        "                       bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Prepare data\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            X = self._prepare_data(data, pbar)\n",
        "\n",
        "            # Step 2: Prepare targets\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Preparing targets...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            y_aqi = data['AQI'].values\n",
        "            y_severity = data['Severity'].map(self.label_encoders['Severity'].transform) if 'Severity' in data.columns else None\n",
        "            y_pollutant = data['Main Pollutant'].map(self.label_encoders['Main Pollutant'].transform) if 'Main Pollutant' in data.columns else None\n",
        "\n",
        "            # Step 3: Setup models\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Setting up models...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            # Choose between cuML (GPU) and scikit-learn based on availability\n",
        "            if self.cuml_available:\n",
        "                print(\"ðŸš€ Using cuML GPU-accelerated Random Forest\")\n",
        "\n",
        "                # cuML Random Forest parameters\n",
        "                rf_params = {\n",
        "                    'n_estimators': self.n_estimators,\n",
        "                    'max_depth': self.max_depth,\n",
        "                    'random_state': self.random_state,\n",
        "                    'bootstrap': True\n",
        "                }\n",
        "\n",
        "                regressor_class = self.cuRFRegressor\n",
        "                classifier_class = self.cuRFClassifier\n",
        "\n",
        "            else:\n",
        "                print(\"ðŸ–¥ï¸  Using scikit-learn Random Forest with CPU acceleration\")\n",
        "\n",
        "                # Scikit-learn Random Forest parameters\n",
        "                rf_params = {\n",
        "                    'n_estimators': self.n_estimators,\n",
        "                    'max_depth': self.max_depth,\n",
        "                    'random_state': self.random_state,\n",
        "                    'n_jobs': self.n_jobs,\n",
        "                    'oob_score': True\n",
        "                }\n",
        "\n",
        "                regressor_class = RandomForestRegressor\n",
        "                classifier_class = RandomForestClassifier\n",
        "\n",
        "            # Step 4: Train AQI regressor\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "\n",
        "            X_train, X_val, y_train_aqi, y_val_aqi = train_test_split(X, y_aqi, test_size=0.2, random_state=42)\n",
        "\n",
        "            self.regressor = regressor_class(**rf_params)\n",
        "            self.regressor = self._train_with_progress(\n",
        "                self.regressor, X_train, y_train_aqi, X_val, y_val_aqi,\n",
        "                \"AQI Regressor\", pbar\n",
        "            )\n",
        "\n",
        "            # Step 5: Train severity classifier\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "\n",
        "            if y_severity is not None:\n",
        "                X_train_sev, X_val_sev, y_train_sev, y_val_sev = train_test_split(\n",
        "                    X, y_severity, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                self.severity_classifier = classifier_class(**rf_params)\n",
        "                self.severity_classifier = self._train_with_progress(\n",
        "                    self.severity_classifier, X_train_sev, y_train_sev, X_val_sev, y_val_sev,\n",
        "                    \"Severity Classifier\", pbar\n",
        "                )\n",
        "\n",
        "            # Step 6: Train pollutant classifier\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "\n",
        "            if y_pollutant is not None:\n",
        "                X_train_pol, X_val_pol, y_train_pol, y_val_pol = train_test_split(\n",
        "                    X, y_pollutant, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                self.pollutant_classifier = classifier_class(**rf_params)\n",
        "                self.pollutant_classifier = self._train_with_progress(\n",
        "                    self.pollutant_classifier, X_train_pol, y_train_pol, X_val_pol, y_val_pol,\n",
        "                    \"Pollutant Classifier\", pbar\n",
        "                )\n",
        "\n",
        "            self.is_fitted = True\n",
        "\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Training complete!\")\n",
        "                pbar.close()\n",
        "\n",
        "            # Display training summary\n",
        "            print(\"âœ… Random Forest models training completed!\")\n",
        "            print(f\"   Acceleration: {'GPU (cuML)' if self.cuml_available else 'CPU (scikit-learn)'}\")\n",
        "            print(f\"   Features: {len(self.feature_columns)}\")\n",
        "            print(f\"   Trees per model: {self.n_estimators}\")\n",
        "\n",
        "            # Model performance\n",
        "            aqi_rmse = np.sqrt(mean_squared_error(y_val_aqi, self.regressor.predict(X_val)))\n",
        "            print(f\"   AQI RMSE: {aqi_rmse:.4f}\")\n",
        "\n",
        "            if hasattr(self.regressor, 'oob_score_'):\n",
        "                print(f\"   AQI OOB Score: {self.regressor.oob_score_:.4f}\")\n",
        "\n",
        "            if self.severity_classifier:\n",
        "                sev_acc = accuracy_score(y_val_sev, self.severity_classifier.predict(X_val_sev))\n",
        "                print(f\"   Severity Accuracy: {sev_acc:.4f}\")\n",
        "\n",
        "            if self.pollutant_classifier:\n",
        "                pol_acc = accuracy_score(y_val_pol, self.pollutant_classifier.predict(X_val_pol))\n",
        "                print(f\"   Pollutant Accuracy: {pol_acc:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if pbar:\n",
        "                pbar.close()\n",
        "            print(f\"âœ— Error training Random Forest models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before making predictions\")\n",
        "\n",
        "        X = self._prepare_data(data)\n",
        "\n",
        "        predictions = {}\n",
        "\n",
        "        # AQI prediction\n",
        "        predictions['aqi'] = self.regressor.predict(X)\n",
        "\n",
        "        # Severity prediction\n",
        "        if self.severity_classifier:\n",
        "            predictions['severity'] = self.severity_classifier.predict(X)\n",
        "\n",
        "        # Pollutant prediction\n",
        "        if self.pollutant_classifier:\n",
        "            predictions['pollutant'] = self.pollutant_classifier.predict(X)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"Get feature importance from trained models\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before getting feature importance\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_columns,\n",
        "            'aqi_importance': self.regressor.feature_importances_\n",
        "        })\n",
        "\n",
        "        if self.severity_classifier:\n",
        "            importance_df['severity_importance'] = self.severity_classifier.feature_importances_\n",
        "\n",
        "        if self.pollutant_classifier:\n",
        "            importance_df['pollutant_importance'] = self.pollutant_classifier.feature_importances_\n",
        "\n",
        "        return importance_df.sort_values('aqi_importance', ascending=False)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the fitted models\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before saving\")\n",
        "\n",
        "        # Convert cuML models to scikit-learn for compatibility if needed\n",
        "        if self.cuml_available:\n",
        "            print(\"âš ï¸  Note: cuML models will be converted to CPU format for saving\")\n",
        "\n",
        "        model_data = {\n",
        "            'regressor': self.regressor,\n",
        "            'severity_classifier': self.severity_classifier,\n",
        "            'pollutant_classifier': self.pollutant_classifier,\n",
        "            'label_encoders': self.label_encoders,\n",
        "            'feature_columns': self.feature_columns,\n",
        "            'is_fitted': self.is_fitted,\n",
        "            'cuml_used': self.cuml_available\n",
        "        }\n",
        "\n",
        "        joblib.dump(model_data, filepath)\n",
        "        print(f\"âœ“ Random Forest models saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load fitted models\"\"\"\n",
        "        model_data = joblib.load(filepath)\n",
        "\n",
        "        self.regressor = model_data['regressor']\n",
        "        self.severity_classifier = model_data['severity_classifier']\n",
        "        self.pollutant_classifier = model_data['pollutant_classifier']\n",
        "        self.label_encoders = model_data['label_encoders']\n",
        "        self.feature_columns = model_data['feature_columns']\n",
        "        self.is_fitted = model_data['is_fitted']\n",
        "\n",
        "        if model_data.get('cuml_used', False):\n",
        "            print(\"â„¹ï¸  This model was trained with cuML GPU acceleration\")\n",
        "\n",
        "        print(f\"âœ“ Random Forest models loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "pTTiwXkRzzE4"
      },
      "id": "pTTiwXkRzzE4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostModel:\n",
        "    \"\"\"\n",
        "    Enhanced XGBoost Model with GPU acceleration and progress bar support for Google Colab\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1, use_gpu=True):\n",
        "        \"\"\"\n",
        "        Initialize XGBoost model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_estimators : int\n",
        "            Number of boosting rounds\n",
        "        max_depth : int\n",
        "            Maximum tree depth\n",
        "        learning_rate : float\n",
        "            Learning rate (eta)\n",
        "        use_gpu : bool\n",
        "            Whether to use GPU acceleration\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        self.regressor = None\n",
        "        self.severity_classifier = None\n",
        "        self.pollutant_classifier = None\n",
        "        self.label_encoders = {}\n",
        "        self.feature_columns = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "        # Setup GPU acceleration\n",
        "        self._setup_gpu()\n",
        "\n",
        "    def _setup_gpu(self):\n",
        "        \"\"\"Setup GPU acceleration for XGBoost\"\"\"\n",
        "        print(\"ðŸ”§ Setting up XGBoost GPU acceleration...\")\n",
        "\n",
        "        if self.use_gpu:\n",
        "            try:\n",
        "                # Check if GPU is available\n",
        "                import subprocess\n",
        "                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"âœ“ NVIDIA GPU detected!\")\n",
        "                    print(\"  GPU acceleration will be used for XGBoost\")\n",
        "\n",
        "                    # Extract GPU info\n",
        "                    gpu_info = result.stdout.split('\\n')[8:10]  # GPU info lines\n",
        "                    for line in gpu_info:\n",
        "                        if 'Tesla' in line or 'GeForce' in line or 'Quadro' in line:\n",
        "                            gpu_name = line.split('|')[1].strip().split()[0:3]\n",
        "                            print(f\"  GPU: {' '.join(gpu_name)}\")\n",
        "                            break\n",
        "                else:\n",
        "                    print(\"âš ï¸  No NVIDIA GPU detected. Using CPU.\")\n",
        "                    self.use_gpu = False\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  GPU detection failed: {e}\")\n",
        "                print(\"   Falling back to CPU mode\")\n",
        "                self.use_gpu = False\n",
        "        else:\n",
        "            print(\"ðŸ–¥ï¸  CPU mode selected\")\n",
        "\n",
        "    def _get_base_params(self):\n",
        "        \"\"\"Get base parameters for XGBoost\"\"\"\n",
        "        params = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_depth': self.max_depth,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        # Add GPU parameters if available\n",
        "        if self.use_gpu:\n",
        "            params.update({\n",
        "                'tree_method': 'gpu_hist',\n",
        "                'gpu_id': 0,\n",
        "                'predictor': 'gpu_predictor'\n",
        "            })\n",
        "        else:\n",
        "            params.update({\n",
        "                'tree_method': 'hist'\n",
        "            })\n",
        "\n",
        "        return params\n",
        "\n",
        "    def _create_features(self, data, progress_bar=None):\n",
        "        \"\"\"Create engineered features\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Engineering features...\")\n",
        "\n",
        "        # Basic features\n",
        "        feature_data = data.copy()\n",
        "\n",
        "        # Time-based features if date column exists\n",
        "        if 'From Date' in data.columns:\n",
        "            feature_data['From Date'] = pd.to_datetime(feature_data['From Date'])\n",
        "            feature_data['hour'] = feature_data['From Date'].dt.hour\n",
        "            feature_data['day_of_week'] = feature_data['From Date'].dt.dayofweek\n",
        "            feature_data['month'] = feature_data['From Date'].dt.month\n",
        "            feature_data['is_weekend'] = (feature_data['day_of_week'] >= 5).astype(int)\n",
        "\n",
        "        # Weather interaction features\n",
        "        if all(col in data.columns for col in ['AT (degree C)', 'RH (%)']):\n",
        "            feature_data['temp_humidity_interaction'] = feature_data['AT (degree C)'] * feature_data['RH (%)']\n",
        "\n",
        "        if all(col in data.columns for col in ['WS (m/s)', 'WD (degree)']):\n",
        "            feature_data['wind_components_x'] = feature_data['WS (m/s)'] * np.cos(np.radians(feature_data['WD (degree)']))\n",
        "            feature_data['wind_components_y'] = feature_data['WS (m/s)'] * np.sin(np.radians(feature_data['WD (degree)']))\n",
        "\n",
        "        # Pollutant ratios\n",
        "        pollutant_cols = ['Benzene (ug/m3)', 'NO (ug/m3)', 'NOx (ug/m3)', 'Toluene (ug/m3)']\n",
        "        available_pollutants = [col for col in pollutant_cols if col in data.columns]\n",
        "\n",
        "        if len(available_pollutants) >= 2:\n",
        "            for i, col1 in enumerate(available_pollutants):\n",
        "                for col2 in available_pollutants[i+1:]:\n",
        "                    ratio_name = f\"{col1.split()[0]}_{col2.split()[0]}_ratio\"\n",
        "                    feature_data[ratio_name] = (feature_data[col1] + 1e-8) / (feature_data[col2] + 1e-8)\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\"Features\": feature_data.shape[1]})\n",
        "\n",
        "        return feature_data\n",
        "\n",
        "    def _prepare_data(self, data, progress_bar=None):\n",
        "        \"\"\"Prepare data for XGBoost training\"\"\"\n",
        "        if progress_bar:\n",
        "            progress_bar.set_description(\"Preparing data...\")\n",
        "\n",
        "        # Create features\n",
        "        feature_data = self._create_features(data, progress_bar)\n",
        "\n",
        "        # Handle missing values\n",
        "        numeric_columns = feature_data.select_dtypes(include=[np.number]).columns\n",
        "        feature_data[numeric_columns] = feature_data[numeric_columns].fillna(feature_data[numeric_columns].median())\n",
        "\n",
        "        # Encode categorical variables\n",
        "        categorical_columns = ['Severity', 'Main Pollutant']\n",
        "        for col in categorical_columns:\n",
        "            if col in feature_data.columns:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    feature_data[col] = self.label_encoders[col].fit_transform(feature_data[col].astype(str))\n",
        "                else:\n",
        "                    feature_data[col] = self.label_encoders[col].transform(feature_data[col].astype(str))\n",
        "\n",
        "        # Select feature columns (exclude targets and non-numeric columns)\n",
        "        exclude_cols = ['From Date', 'file_name', 'state', 'city']\n",
        "        self.feature_columns = [col for col in feature_data.columns\n",
        "                               if col not in exclude_cols and feature_data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "        if progress_bar:\n",
        "            progress_bar.set_postfix({\n",
        "                \"Final features\": len(self.feature_columns),\n",
        "                \"Samples\": len(feature_data)\n",
        "            })\n",
        "\n",
        "        return feature_data[self.feature_columns]\n",
        "\n",
        "    def fit(self, data, show_progress=True):\n",
        "        \"\"\"\n",
        "        Fit XGBoost models with GPU acceleration and progress tracking\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pd.DataFrame\n",
        "            Training data\n",
        "        show_progress : bool\n",
        "            Whether to show progress bar\n",
        "        \"\"\"\n",
        "        # Initialize progress bar\n",
        "        progress_steps = 6\n",
        "        if show_progress:\n",
        "            pbar = tqdm(total=progress_steps, desc=\"Training XGBoost\",\n",
        "                       bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Prepare data\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            X = self._prepare_data(data, pbar)\n",
        "\n",
        "            # Step 2: Prepare targets\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Preparing targets...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            y_aqi = data['AQI'].values\n",
        "            y_severity = data['Severity'].map(self.label_encoders['Severity'].transform) if 'Severity' in data.columns else None\n",
        "            y_pollutant = data['Main Pollutant'].map(self.label_encoders['Main Pollutant'].transform) if 'Main Pollutant' in data.columns else None\n",
        "            print(y_aqi.shape)\n",
        "            # Step 3: Setup base parameters\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Configuring models...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            base_params = self._get_base_params()\n",
        "\n",
        "            # Step 4: Train AQI regressor\n",
        "            if pbar:\n",
        "                pbar.set_description(f\"Training AQI regressor ({'GPU' if self.use_gpu else 'CPU'})...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            print(f\"ðŸš€ Training AQI regressor on {'GPU' if self.use_gpu else 'CPU'}...\")\n",
        "\n",
        "            regressor_params = base_params.copy()\n",
        "            regressor_params.update({\n",
        "                'objective': 'reg:squarederror',\n",
        "                'eval_metric': 'rmse'\n",
        "            })\n",
        "\n",
        "            # Custom callback for progress tracking\n",
        "            def callback_progress(env):\n",
        "                if env.iteration % 10 == 0:\n",
        "                    print(f\"   Iteration {env.iteration}: RMSE = {env.evaluation_result_list[0][1]:.4f}\")\n",
        "\n",
        "            # Train with validation and early stopping\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X, y_aqi, test_size=0.2, random_state=42)\n",
        "\n",
        "            self.regressor = xgb.XGBRegressor(**regressor_params)\n",
        "            self.regressor.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                early_stopping_rounds=10,\n",
        "                verbose=True if show_progress else False\n",
        "            )\n",
        "\n",
        "            # Step 5: Train severity classifier\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Training severity classifier...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            if y_severity is not None:\n",
        "                print(\"ðŸŽ¯ Training severity classifier...\")\n",
        "                classifier_params = base_params.copy()\n",
        "                classifier_params.update({\n",
        "                    'objective': 'multi:softprob',\n",
        "                    'eval_metric': 'mlogloss',\n",
        "                    'num_class': len(self.label_encoders['Severity'].classes_)\n",
        "                })\n",
        "\n",
        "                X_train_sev, X_val_sev, y_train_sev, y_val_sev = train_test_split(\n",
        "                    X, y_severity, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                self.severity_classifier = xgb.XGBClassifier(**classifier_params)\n",
        "                self.severity_classifier.fit(\n",
        "                    X_train_sev, y_train_sev,\n",
        "                    eval_set=[(X_val_sev, y_val_sev)],\n",
        "                    early_stopping_rounds=10,\n",
        "                    verbose=False\n",
        "                )\n",
        "\n",
        "            # Step 6: Train pollutant classifier\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Training pollutant classifier...\")\n",
        "                pbar.update(1)\n",
        "\n",
        "            if y_pollutant is not None:\n",
        "                print(\"ðŸ­ Training pollutant classifier...\")\n",
        "                classifier_params = base_params.copy()\n",
        "                classifier_params.update({\n",
        "                    'objective': 'multi:softprob',\n",
        "                    'eval_metric': 'mlogloss',\n",
        "                    'num_class': len(self.label_encoders['Main Pollutant'].classes_)\n",
        "                })\n",
        "\n",
        "                X_train_pol, X_val_pol, y_train_pol, y_val_pol = train_test_split(\n",
        "                    X, y_pollutant, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                self.pollutant_classifier = xgb.XGBClassifier(**classifier_params)\n",
        "                self.pollutant_classifier.fit(\n",
        "                    X_train_pol, y_train_pol,\n",
        "                    eval_set=[(X_val_pol, y_val_pol)],\n",
        "                    early_stopping_rounds=10,\n",
        "                    verbose=False\n",
        "                )\n",
        "\n",
        "            self.is_fitted = True\n",
        "\n",
        "            if pbar:\n",
        "                pbar.set_description(\"Training complete!\")\n",
        "                pbar.close()\n",
        "\n",
        "            # Display training summary\n",
        "            print(\"âœ“ XGBoost models training completed!\")\n",
        "            print(f\"  Device: {'GPU' if self.use_gpu else 'CPU'}\")\n",
        "            print(f\"  Features: {len(self.feature_columns)}\")\n",
        "            print(f\"  AQI RMSE: {np.sqrt(mean_squared_error(y_val, self.regressor.predict(X_val))):.4f}\")\n",
        "\n",
        "            if self.severity_classifier:\n",
        "                sev_acc = accuracy_score(y_val_sev, self.severity_classifier.predict(X_val_sev))\n",
        "                print(f\"  Severity Accuracy: {sev_acc:.4f}\")\n",
        "\n",
        "            if self.pollutant_classifier:\n",
        "                pol_acc = accuracy_score(y_val_pol, self.pollutant_classifier.predict(X_val_pol))\n",
        "                print(f\"  Pollutant Accuracy: {pol_acc:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if pbar:\n",
        "                pbar.close()\n",
        "            print(f\"âœ— Error training XGBoost models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before making predictions\")\n",
        "\n",
        "        X = self._prepare_data(data)\n",
        "\n",
        "        predictions = {}\n",
        "\n",
        "        # AQI prediction\n",
        "        predictions['aqi'] = self.regressor.predict(X)\n",
        "\n",
        "        # Severity prediction\n",
        "        if self.severity_classifier:\n",
        "            predictions['severity'] = self.severity_classifier.predict(X)\n",
        "\n",
        "        # Pollutant prediction\n",
        "        if self.pollutant_classifier:\n",
        "            predictions['pollutant'] = self.pollutant_classifier.predict(X)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"Get feature importance from trained models\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before getting feature importance\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_columns,\n",
        "            'aqi_importance': self.regressor.feature_importances_\n",
        "        })\n",
        "\n",
        "        if self.severity_classifier:\n",
        "            importance_df['severity_importance'] = self.severity_classifier.feature_importances_\n",
        "\n",
        "        if self.pollutant_classifier:\n",
        "            importance_df['pollutant_importance'] = self.pollutant_classifier.feature_importances_\n",
        "\n",
        "        return importance_df.sort_values('aqi_importance', ascending=False)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the fitted models\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Models must be fitted before saving\")\n",
        "\n",
        "        model_data = {\n",
        "            'regressor': self.regressor,\n",
        "            'severity_classifier': self.severity_classifier,\n",
        "            'pollutant_classifier': self.pollutant_classifier,\n",
        "            'label_encoders': self.label_encoders,\n",
        "            'feature_columns': self.feature_columns,\n",
        "            'is_fitted': self.is_fitted\n",
        "        }\n",
        "\n",
        "        joblib.dump(model_data, filepath)\n",
        "        print(f\"âœ“ XGBoost models saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load fitted models\"\"\"\n",
        "        model_data = joblib.load(filepath)\n",
        "\n",
        "        self.regressor = model_data['regressor']\n",
        "        self.severity_classifier = model_data['severity_classifier']\n",
        "        self.pollutant_classifier = model_data['pollutant_classifier']\n",
        "        self.label_encoders = model_data['label_encoders']\n",
        "        self.feature_columns = model_data['feature_columns']\n",
        "        self.is_fitted = model_data['is_fitted']\n",
        "\n",
        "        print(f\"âœ“ XGBoost models loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "fLLl1efaz3Y0"
      },
      "id": "fLLl1efaz3Y0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_colab_environment():\n",
        "    \"\"\"Setup Google Colab environment with GPU and required packages\"\"\"\n",
        "    print(\"ðŸ”§ Setting up Google Colab environment...\")\n",
        "\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        in_colab = True\n",
        "        print(\"âœ“ Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        in_colab = False\n",
        "        print(\"â„¹ï¸  Not running in Google Colab\")\n",
        "\n",
        "    if in_colab:\n",
        "        print(\"ðŸ“‹ Installing required packages for Colab...\")\n",
        "\n",
        "        # Install packages that might not be available in Colab\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        packages = [\n",
        "            'tqdm',\n",
        "            'plotly',\n",
        "            'statsmodels'\n",
        "        ]\n",
        "\n",
        "        for package in packages:\n",
        "            try:\n",
        "                __import__(package)\n",
        "                print(f\"âœ“ {package} already installed\")\n",
        "            except ImportError:\n",
        "                print(f\"ðŸ“¦ Installing {package}...\")\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "        # Optional: Install cuML for GPU-accelerated Random Forest\n",
        "        try:\n",
        "            import cuml\n",
        "            print(\"âœ“ cuML (GPU acceleration) is available\")\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸  cuML not available. For GPU Random Forest, install with:\")\n",
        "            print(\"   !pip install cuml-cu11\")\n",
        "\n",
        "    # Check GPU availability\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "    if gpus:\n",
        "        print(f\"ðŸš€ GPU acceleration available!\")\n",
        "        print(f\"   GPUs detected: {len(gpus)}\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"   GPU {i}: {gpu.name}\")\n",
        "\n",
        "        # Configure GPU memory growth\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "        return True\n",
        "    else:\n",
        "        print(\"âš ï¸  No GPU detected. Models will run on CPU.\")\n",
        "        print(\"   To enable GPU in Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "        return False\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the AQI dataset\"\"\"\n",
        "    print(\"ðŸ“Š Loading and preprocessing data...\")\n",
        "\n",
        "    # Load data\n",
        "    if file_path.endswith('.csv'):\n",
        "        data = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV file.\")\n",
        "\n",
        "    print(f\"   Original data shape: {data.shape}\")\n",
        "\n",
        "    # Basic preprocessing\n",
        "    if 'From Date' in data.columns:\n",
        "        data['From Date'] = pd.to_datetime(data['From Date'])\n",
        "        data = data.sort_values('From Date')\n",
        "\n",
        "    # Handle missing values in critical columns\n",
        "    if 'AQI' in data.columns:\n",
        "        data = data.dropna(subset=['AQI'])\n",
        "\n",
        "    # Fill missing values for other columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "    print(f\"   Processed data shape: {data.shape}\")\n",
        "    print(f\"   Date range: {data['From Date'].min()} to {data['From Date'].max()}\")\n",
        "    print(f\"   AQI range: {data['AQI'].min():.1f} to {data['AQI'].max():.1f}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
        "    print(\"âœ‚ï¸  Splitting data...\")\n",
        "\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
        "\n",
        "    n = len(data)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "\n",
        "    train_data = data.iloc[:train_end]\n",
        "    val_data = data.iloc[train_end:val_end]\n",
        "    test_data = data.iloc[val_end:]\n",
        "\n",
        "    print(f\"   Train set: {len(train_data)} samples ({train_ratio*100:.1f}%)\")\n",
        "    print(f\"   Validation set: {len(val_data)} samples ({val_ratio*100:.1f}%)\")\n",
        "    print(f\"   Test set: {len(test_data)} samples ({test_ratio*100:.1f}%)\")\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def train_models(train_data, val_data, base_path, gpu_available=False, show_progress=True):\n",
        "    \"\"\"Train all models with progress tracking\"\"\"\n",
        "    print(\"ðŸƒâ€â™‚ï¸ Starting model training...\")\n",
        "\n",
        "    # Create models directory\n",
        "    # Path(\"models\").mkdir(exist_ok=True)\n",
        "\n",
        "    models = {}\n",
        "    training_results = {}\n",
        "\n",
        "    # Progress bar for overall training\n",
        "    model_count = 4\n",
        "    if show_progress:\n",
        "        overall_pbar = tqdm(total=model_count, desc=\"Training Models\",\n",
        "                           bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
        "\n",
        "    # 1. Train ARIMA Model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"1ï¸âƒ£  TRAINING ARIMA MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        arima_model = ARIMAModel(order=(1, 1, 1))\n",
        "        start_time = time.time()\n",
        "        arima_model.fit(train_data['AQI'], show_progress=show_progress)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        arima_model.save_model(base_path + 'arima_model.pkl')\n",
        "\n",
        "        # Make predictions\n",
        "        val_pred = arima_model.predict(steps=len(val_data))\n",
        "\n",
        "        models['arima'] = arima_model\n",
        "        training_results['arima'] = {\n",
        "            'training_time': training_time,\n",
        "            'val_predictions': val_pred\n",
        "        }\n",
        "\n",
        "        print(f\"âœ“ ARIMA training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— ARIMA training failed: {e}\")\n",
        "\n",
        "    if show_progress:\n",
        "        overall_pbar.update(1)\n",
        "\n",
        "    # 2. Train LSTM Model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"2ï¸âƒ£  TRAINING LSTM MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        lstm_model = LSTMModel(sequence_length=24, lstm_units=50)\n",
        "        start_time = time.time()\n",
        "        history = lstm_model.fit(train_data, epochs=20, show_progress=show_progress)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        lstm_model.save_model(base_path + 'lstm_model.h5')\n",
        "\n",
        "        # Make predictions\n",
        "        val_pred = lstm_model.predict(val_data)\n",
        "\n",
        "        models['lstm'] = lstm_model\n",
        "        training_results['lstm'] = {\n",
        "            'training_time': training_time,\n",
        "            'val_predictions': val_pred,\n",
        "            'history': history\n",
        "        }\n",
        "\n",
        "        print(f\"âœ“ LSTM training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— LSTM training failed: {e}\")\n",
        "\n",
        "    if show_progress:\n",
        "        overall_pbar.update(1)\n",
        "\n",
        "    # 3. Train XGBoost Model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"3ï¸âƒ£  TRAINING XGBOOST MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        xgb_model = XGBoostModel(n_estimators=100, use_gpu=gpu_available)\n",
        "        start_time = time.time()\n",
        "        xgb_model.fit(train_data, show_progress=show_progress)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        xgb_model.save_model(base_path + 'xgboost_model.pkl')\n",
        "\n",
        "        # Make predictions\n",
        "        val_pred = xgb_model.predict(val_data)\n",
        "\n",
        "        models['xgboost'] = xgb_model\n",
        "        training_results['xgboost'] = {\n",
        "            'training_time': training_time,\n",
        "            'val_predictions': val_pred\n",
        "        }\n",
        "\n",
        "        print(f\"âœ“ XGBoost training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— XGBoost training failed: {e}\")\n",
        "\n",
        "    if show_progress:\n",
        "        overall_pbar.update(1)\n",
        "\n",
        "    # 4. Train Random Forest Model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"4ï¸âƒ£  TRAINING RANDOM FOREST MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        rf_model = RandomForestModel(n_estimators=100, use_gpu_alternative=gpu_available)\n",
        "        start_time = time.time()\n",
        "        rf_model.fit(train_data, show_progress=show_progress)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        rf_model.save_model(base_path + 'random_forest_model.pkl')\n",
        "\n",
        "        # Make predictions\n",
        "        val_pred = rf_model.predict(val_data)\n",
        "\n",
        "        models['random_forest'] = rf_model\n",
        "        training_results['random_forest'] = {\n",
        "            'training_time': training_time,\n",
        "            'val_predictions': val_pred\n",
        "        }\n",
        "\n",
        "        print(f\"âœ“ Random Forest training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Random Forest training failed: {e}\")\n",
        "\n",
        "    if show_progress:\n",
        "        overall_pbar.update(1)\n",
        "        overall_pbar.close()\n",
        "\n",
        "    return models, training_results\n",
        "\n",
        "def evaluate_models(models, training_results, val_data, test_data):\n",
        "    \"\"\"Evaluate all trained models\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“Š MODEL EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    evaluation_results = {}\n",
        "\n",
        "    print(f\"{'Model':<15} {'AQI RMSE':<10} {'AQI MAE':<10} {'AQI RÂ²':<10} {'Training Time':<15}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for model_name, result in training_results.items():\n",
        "        if model_name not in models:\n",
        "            continue\n",
        "\n",
        "        model = models[model_name]\n",
        "        training_time = result['training_time']\n",
        "\n",
        "        try:\n",
        "            if model_name == 'arima':\n",
        "                # ARIMA predictions\n",
        "                val_pred_aqi = result['val_predictions'][:len(val_data)]\n",
        "                val_true_aqi = val_data['AQI'].values[:len(val_pred_aqi)]\n",
        "            else:\n",
        "                # Other models\n",
        "                val_pred = result['val_predictions']\n",
        "                val_pred_aqi = val_pred['aqi'] if isinstance(val_pred, dict) else val_pred\n",
        "                val_true_aqi = val_data['AQI'].values\n",
        "\n",
        "                # Ensure same length\n",
        "                min_len = min(len(val_pred_aqi), len(val_true_aqi))\n",
        "                val_pred_aqi = val_pred_aqi[:min_len]\n",
        "                val_true_aqi = val_true_aqi[:min_len]\n",
        "\n",
        "            # Calculate metrics\n",
        "            rmse = np.sqrt(mean_squared_error(val_true_aqi, val_pred_aqi))\n",
        "            mae = mean_absolute_error(val_true_aqi, val_pred_aqi)\n",
        "            r2 = r2_score(val_true_aqi, val_pred_aqi)\n",
        "\n",
        "            evaluation_results[model_name] = {\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'r2': r2,\n",
        "                'training_time': training_time\n",
        "            }\n",
        "\n",
        "            print(f\"{model_name:<15} {rmse:<10.2f} {mae:<10.2f} {r2:<10.3f} {training_time:<15.2f}s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name:<15} {'ERROR':<10} {'ERROR':<10} {'ERROR':<10} {training_time:<15.2f}s\")\n",
        "            print(f\"   Error: {e}\")\n",
        "\n",
        "    # Find best model\n",
        "    if evaluation_results:\n",
        "        best_model = min(evaluation_results.keys(), key=lambda x: evaluation_results[x]['rmse'])\n",
        "        print(f\"\\nðŸ† Best model: {best_model} (RMSE: {evaluation_results[best_model]['rmse']:.2f})\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "def create_training_summary(evaluation_results, gpu_available):\n",
        "    \"\"\"Create a summary of the training session\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“‹ TRAINING SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"Environment: {'GPU Accelerated' if gpu_available else 'CPU Only'}\")\n",
        "    print(f\"Models trained: {len(evaluation_results)}\")\n",
        "    print(f\"Total training time: {sum(r['training_time'] for r in evaluation_results.values()):.2f} seconds\")\n",
        "\n",
        "    if evaluation_results:\n",
        "        # Performance ranking\n",
        "        sorted_models = sorted(evaluation_results.items(), key=lambda x: x[1]['rmse'])\n",
        "\n",
        "        print(\"\\nPerformance Ranking (by RMSE):\")\n",
        "        for i, (model_name, results) in enumerate(sorted_models, 1):\n",
        "            print(f\"  {i}. {model_name}: {results['rmse']:.2f}\")\n",
        "\n",
        "        # Speed ranking\n",
        "        sorted_by_speed = sorted(evaluation_results.items(), key=lambda x: x[1]['training_time'])\n",
        "\n",
        "        print(\"\\nSpeed Ranking (by training time):\")\n",
        "        for i, (model_name, results) in enumerate(sorted_by_speed, 1):\n",
        "            print(f\"  {i}. {model_name}: {results['training_time']:.2f}s\")\n",
        "\n",
        "def main(data_file):\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    print(\"ðŸš€ AQI PREDICTION MODEL TRAINING PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Setup environment\n",
        "    gpu_available = setup_colab_environment()\n",
        "\n",
        "    # Load data\n",
        "    data = load_and_preprocess_data(data_file)\n",
        "\n",
        "    # Split data\n",
        "    train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "    # Train models\n",
        "    models, training_results = train_models(train_data, val_data, '/content/drive/MyDrive/ML_Dataset/AQI_2010_2023_updated/' ,gpu_available)\n",
        "\n",
        "    # Evaluate models\n",
        "    evaluation_results = evaluate_models(models, training_results, val_data, test_data)\n",
        "\n",
        "    # Create summary\n",
        "    create_training_summary(evaluation_results, gpu_available)\n",
        "\n",
        "    print(\"\\nâœ… Training pipeline completed!\")\n",
        "    print(f\"ðŸ“ Models saved in: {Path('models').absolute()}\")\n",
        "\n",
        "    return models, evaluation_results"
      ],
      "metadata": {
        "id": "w6vTb6fJ0acX"
      },
      "id": "w6vTb6fJ0acX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_aqi(from_datetime, to_datetime, location_city, model_type='xgboost'):\n",
        "    \"\"\"\n",
        "    Predict AQI for a given time period and location\n",
        "\n",
        "    Parameters:\n",
        "    from_datetime: Start datetime (str or datetime)\n",
        "    to_datetime: End datetime (str or datetime)\n",
        "    location_city: City name (str)\n",
        "    model_type: Type of model to use ('arima', 'lstm', 'xgboost', 'random_forest')\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with predictions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert datetime strings if needed\n",
        "        if isinstance(from_datetime, str):\n",
        "            from_datetime = pd.to_datetime(from_datetime)\n",
        "        if isinstance(to_datetime, str):\n",
        "            to_datetime = pd.to_datetime(to_datetime)\n",
        "\n",
        "        # Create hourly datetime range\n",
        "        datetime_range = pd.date_range(start=from_datetime, end=to_datetime, freq='H')\n",
        "\n",
        "        # Create dummy data for prediction (in real scenario, you'd have actual sensor data)\n",
        "        # For demonstration, we'll use sample data patterns\n",
        "        n_hours = len(datetime_range)\n",
        "\n",
        "        # Create base DataFrame\n",
        "        prediction_data = pd.DataFrame({\n",
        "            'From Date': datetime_range,\n",
        "            'AQI': np.random.randint(30, 100, n_hours),  # Dummy AQI values\n",
        "            'AT (degree C)': np.random.normal(25, 5, n_hours),\n",
        "            'RH (%)': np.random.normal(65, 10, n_hours),\n",
        "            'WS (m/s)': np.random.normal(2, 0.5, n_hours),\n",
        "            'WD (degree)': np.random.uniform(0, 360, n_hours),\n",
        "            'SR (W/mt2)': np.random.uniform(0, 500, n_hours),\n",
        "            'NO (ug/m3)': np.random.uniform(10, 50, n_hours),\n",
        "            'NOx (ug/m3)': np.random.uniform(20, 80, n_hours),\n",
        "            'Benzene (ug/m3)': np.random.uniform(0.5, 2, n_hours),\n",
        "            'Toluene (ug/m3)': np.random.uniform(2, 8, n_hours),\n",
        "            'RF (mm)': np.random.uniform(0, 2, n_hours),\n",
        "            'BP (mmHg)': np.random.normal(760, 20, n_hours),\n",
        "            'Severity': np.random.choice(['GOOD', 'SATISFACTORY', 'MODERATE', 'POOR'], n_hours),\n",
        "            'Main Pollutant': np.random.choice(['PM10 (ug/m3)', 'PM2.5 (ug/m3)', 'NO2 (ug/m3)', 'CO (mg/m3)'], n_hours),\n",
        "            'city': location_city,\n",
        "            'state': 'Unknown',\n",
        "            'latitude': 0.0,\n",
        "            'longitude': 0.0,\n",
        "            'elevation': 0.0,\n",
        "            'file_name': 'PRED001'\n",
        "        })\n",
        "\n",
        "        # Load appropriate model\n",
        "        models_dir = 'models'\n",
        "        if model_type == 'arima':\n",
        "            model = ARIMAModel()\n",
        "            model.load_model(os.path.join(models_dir, 'arima_model.pkl'))\n",
        "            # For ARIMA, we need to implement a different prediction approach\n",
        "            forecast, conf_int = model.predict(steps=n_hours)\n",
        "            predictions = pd.DataFrame({\n",
        "                'datetime': datetime_range,\n",
        "                'AQI': forecast,\n",
        "                'Severity': 'PREDICTED',\n",
        "                'Main_Pollutant': 'PREDICTED'\n",
        "            })\n",
        "\n",
        "        elif model_type == 'lstm':\n",
        "            model = LSTMModel()\n",
        "            model.load_model(os.path.join(models_dir, 'lstm_model'))\n",
        "            aqi_pred, severity_pred, pollutant_pred = model.predict(prediction_data)\n",
        "            predictions = pd.DataFrame({\n",
        "                'datetime': datetime_range,\n",
        "                'AQI': aqi_pred,\n",
        "                'Severity': severity_pred,\n",
        "                'Main_Pollutant': pollutant_pred\n",
        "            })\n",
        "\n",
        "        elif model_type == 'xgboost':\n",
        "            model = XGBoostModel()\n",
        "            model.load_model(os.path.join(models_dir, 'xgboost_model'))\n",
        "            aqi_pred, severity_pred, pollutant_pred = model.predict(prediction_data)\n",
        "            predictions = pd.DataFrame({\n",
        "                'datetime': datetime_range,\n",
        "                'AQI': aqi_pred,\n",
        "                'Severity': severity_pred,\n",
        "                'Main_Pollutant': pollutant_pred\n",
        "            })\n",
        "\n",
        "        elif model_type == 'random_forest':\n",
        "            model = RandomForestModel()\n",
        "            model.load_model(os.path.join(models_dir, 'random_forest_model'))\n",
        "            aqi_pred, severity_pred, pollutant_pred, _, _ = model.predict(prediction_data)\n",
        "            predictions = pd.DataFrame({\n",
        "                'datetime': datetime_range,\n",
        "                'AQI': aqi_pred,\n",
        "                'Severity': severity_pred,\n",
        "                'Main_Pollutant': pollutant_pred\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        print(f\"Predictions generated for {location_city} from {from_datetime} to {to_datetime}\")\n",
        "        print(f\"Model used: {model_type.upper()}\")\n",
        "        print(f\"Number of predictions: {len(predictions)}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in prediction: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Pp_0Rezy05y1"
      },
      "id": "Pp_0Rezy05y1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize trainer\n",
        "    trainer = AQIModelTrainer(data_path='/content/drive/MyDrive/ML_Dataset/AQI_2010_2023_updated/statewise_aqi/Andhra Pradesh_aqi.csv',models_dir='/content/drive/MyDrive/ML_Dataset/AQI_2010_2023_updated/models')\n",
        "\n",
        "    # Run complete pipeline\n",
        "    results = trainer.run_complete_pipeline()\n",
        "\n",
        "    # Example prediction\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"EXAMPLE PREDICTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Make a sample prediction\n",
        "    sample_prediction = predict_aqi(\n",
        "        from_datetime='2024-01-01 00:00:00',\n",
        "        to_datetime='2024-01-01 23:00:00',\n",
        "        location_city='Tirupati',\n",
        "        model_type='xgboost'\n",
        "    )\n",
        "\n",
        "    if sample_prediction is not None:\n",
        "        print(\"\\nSample predictions:\")\n",
        "        print(sample_prediction.head(10))"
      ],
      "metadata": {
        "id": "_G2p4cKE07lj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f53ef65075f1498297b9d9f0c8884a26",
            "c57b17cad1be4849a110bda24db2e716",
            "1be1bc550b504aa5b4f7f0eb363fc667",
            "dfcf08f1e6fa41bbbd5621f54fe39a36",
            "40747e07a50148bdb338f331dee3260b"
          ]
        },
        "outputId": "7fcbcb64-ff52-494f-c27b-8967975af441"
      },
      "id": "_G2p4cKE07lj",
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ AQI PREDICTION MODEL TRAINING PIPELINE\n",
            "============================================================\n",
            "ðŸ”§ Setting up Google Colab environment...\n",
            "âœ“ Running in Google Colab\n",
            "ðŸ“‹ Installing required packages for Colab...\n",
            "âœ“ tqdm already installed\n",
            "âœ“ plotly already installed\n",
            "âœ“ statsmodels already installed\n",
            "ðŸš€ GPU acceleration available!\n",
            "   GPUs detected: 1\n",
            "   GPU 0: /physical_device:GPU:0\n",
            "ðŸ“Š Loading and preprocessing data...\n",
            "   Original data shape: (224868, 21)\n",
            "   Processed data shape: (224868, 21)\n",
            "   Date range: 2016-07-01 10:00:00 to 2023-03-31 23:00:00\n",
            "   AQI range: 0.0 to 500.0\n",
            "âœ‚ï¸  Splitting data...\n",
            "   Train set: 157407 samples (70.0%)\n",
            "   Validation set: 33730 samples (15.0%)\n",
            "   Test set: 33731 samples (15.0%)\n",
            "ðŸƒâ€â™‚ï¸ Starting model training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f53ef65075f1498297b9d9f0c8884a26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training Models:   0%|           | 0/4 [00:00<?]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "1ï¸âƒ£  TRAINING ARIMA MODEL\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c57b17cad1be4849a110bda24db2e716",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training ARIMA:   0%|           | 0/4 [00:00<?]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ ARIMA(1, 1, 1) model fitted successfully!\n",
            "  AIC: 1612107.68\n",
            "  Data points: 157407\n",
            "âœ“ ARIMA model saved to /content/drive/MyDrive/ML_Dataset/AQI_2010_2023_updated/modelsarima_model.pkl\n",
            "âœ“ ARIMA training completed in 90.10 seconds\n",
            "\n",
            "============================================================\n",
            "2ï¸âƒ£  TRAINING LSTM MODEL\n",
            "============================================================\n",
            "ðŸ”§ Setting up GPU acceleration for Google Colab...\n",
            "âœ“ GPU acceleration enabled!\n",
            "  Available GPUs: 1\n",
            "  GPU Names: ['/physical_device:GPU:0']\n",
            "  Mixed precision: mixed_float16\n",
            "  GPU Details: Tesla T4\n",
            "AQI                         int64\n",
            "AT (degree C)             float64\n",
            "BP (mmHg)                 float64\n",
            "Benzene (ug/m3)           float64\n",
            "From Date          datetime64[ns]\n",
            "Main Pollutant             object\n",
            "NO (ug/m3)                float64\n",
            "NOx (ug/m3)               float64\n",
            "RF (mm)                   float64\n",
            "RH (%)                    float64\n",
            "SR (W/mt2)                float64\n",
            "Severity                   object\n",
            "Toluene (ug/m3)           float64\n",
            "WD (degree)               float64\n",
            "WS (m/s)                  float64\n",
            "file_name                  object\n",
            "state                      object\n",
            "city                       object\n",
            "latitude                  float64\n",
            "longitude                 float64\n",
            "elevation                 float64\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1be1bc550b504aa5b4f7f0eb363fc667",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training LSTM:   0%|           | 0/5 [00:00<?]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ— Error training LSTM model: float() argument must be a string or a real number, not 'Timestamp'\n",
            "âœ— LSTM training failed: float() argument must be a string or a real number, not 'Timestamp'\n",
            "\n",
            "============================================================\n",
            "3ï¸âƒ£  TRAINING XGBOOST MODEL\n",
            "============================================================\n",
            "ðŸ”§ Setting up XGBoost GPU acceleration...\n",
            "âœ“ NVIDIA GPU detected!\n",
            "  GPU acceleration will be used for XGBoost\n",
            "  GPU: 0 Tesla T4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfcf08f1e6fa41bbbd5621f54fe39a36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training XGBoost:   0%|           | 0/6 [00:00<?]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ— Error training XGBoost models: y should be a 1d array, got an array of shape () instead.\n",
            "âœ— XGBoost training failed: y should be a 1d array, got an array of shape () instead.\n",
            "\n",
            "============================================================\n",
            "4ï¸âƒ£  TRAINING RANDOM FOREST MODEL\n",
            "============================================================\n",
            "ðŸ”§ Setting up Random Forest with acceleration options...\n",
            "âš ï¸  cuML not available. Install with: pip install cuml-cu11\n",
            "   Using standard scikit-learn Random Forest\n",
            "ðŸ“Š Using all 2 CPU cores for parallel processing\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40747e07a50148bdb338f331dee3260b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training Random Forest:   0%|           | 0/6 [00:00<?]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ— Error training Random Forest models: y should be a 1d array, got an array of shape () instead.\n",
            "âœ— Random Forest training failed: y should be a 1d array, got an array of shape () instead.\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š MODEL EVALUATION\n",
            "============================================================\n",
            "Model           AQI RMSE   AQI MAE    AQI RÂ²     Training Time  \n",
            "----------------------------------------------------------------------\n",
            "arima           71.36      50.84      -0.951     90.10          s\n",
            "\n",
            "ðŸ† Best model: arima (RMSE: 71.36)\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ TRAINING SUMMARY\n",
            "============================================================\n",
            "Environment: GPU Accelerated\n",
            "Models trained: 1\n",
            "Total training time: 90.10 seconds\n",
            "\n",
            "Performance Ranking (by RMSE):\n",
            "  1. arima: 71.36\n",
            "\n",
            "Speed Ranking (by training time):\n",
            "  1. arima: 90.10s\n",
            "\n",
            "âœ… Training pipeline completed!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-4156149023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ML_Dataset/AQI_2010_2023_updated/statewise_aqi/Andhra Pradesh_aqi.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-497442639.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_file)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ… Training pipeline completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ðŸ“ Models saved in: {Path('models').absolute()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}